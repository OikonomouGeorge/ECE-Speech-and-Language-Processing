{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30822,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Description\n",
        "\n",
        "In this lab, you will learn how to fine-tune the LLaMA 3.2 3B model using two distinct methods: Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). The lab is structured to provide you with hands-on experience in adapting a large language model to specific tasks by:\n",
        "\n",
        "1. Setting up the environment and preparing the necessary tools and libraries.\n",
        "2. Loading the pre-trained LLaMA 3.2 3B model.\n",
        "3. Performing supervised fine-tuning (SFT) to adapt the model to a dataset.\n",
        "4. Applying Direct Preference Optimization (DPO) to further refine the model's behavior based on preference annotations.\n",
        "5. Evaluating and comparing the performance of the two fine-tuned models on a shared task.\n",
        "\n",
        "You will explore the fundamental steps required for fine-tuning large language models while understanding the differences between SFT and DPO approaches. Additionally, the lab introduces evaluation strategies to measure and compare model performance.\n",
        "\n",
        "While this notebook guides you through the essential steps, additional tasks are provided as exercises for deeper exploration and application of the learned concepts.\n",
        "\n",
        "This [notebook](https://www.kaggle.com/code/danielhanchen/kaggle-llama-3-2-1b-3b-conversational-unsloth) was used as a basis for the current work."
      ],
      "metadata": {
        "id": "Bg2WSL0EMXck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install Required Libraries\n",
        "\n",
        "To begin, we need to set up the environment by installing all necessary libraries and dependencies. This ensures that we have the appropriate versions of PyTorch, transformers, and other tools required for fine-tuning the LLaMA model."
      ],
      "metadata": {
        "id": "xA_sZ0s0MXcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pip3-autoremove\n",
        "\n",
        "!pip-autoremove torch torchvision torchaudio -y\n",
        "\n",
        "!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "!pip install unsloth\n",
        "\n",
        "# dependency conflicts\n",
        "# tensorflow-metadata, ydf, grpcio-status need newer protobuf\n",
        "# ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1\n",
        "!pip install \"protobuf>=5.29.1,<6.0.0\" --upgrade\n",
        "!pip install \"fsspec==2025.3.2\" --upgrade\n",
        "\n",
        "!pip install evaluate\n",
        "\n",
        "!pip install accelerate --upgrade"
      ],
      "metadata": {
        "trusted": true,
        "id": "qyCKK5BuMXcq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "69b4e958-c132-48c6-f854-67f3c5bc05af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip3-autoremove in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (from pip3-autoremove) (24.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from pip3-autoremove) (75.2.0)\n",
            "fsspec 2025.3.0 is installed but fsspec==2025.3.2 is required\n",
            "Redoing requirement with just package name...\n",
            "The 'jedi>=0.16' distribution was not found and is required by the application\n",
            "Skipping jedi\n",
            "protobuf 5.29.4 is installed but protobuf<4.0.0 is required\n",
            "Redoing requirement with just package name...\n",
            "protobuf 5.29.4 is installed but protobuf<4.0.0 is required\n",
            "Redoing requirement with just package name...\n",
            "torchvision 0.20.1+cu121 (/usr/local/lib/python3.11/dist-packages)\n",
            "    torch 2.5.1+cu121 (/usr/local/lib/python3.11/dist-packages)\n",
            "        nvidia-cuda-nvrtc-cu12 12.1.105 (/usr/local/lib/python3.11/dist-packages)\n",
            "        nvidia-cuda-runtime-cu12 12.1.105 (/usr/local/lib/python3.11/dist-packages)\n",
            "        nvidia-cuda-cupti-cu12 12.1.105 (/usr/local/lib/python3.11/dist-packages)\n",
            "        nvidia-cudnn-cu12 9.1.0.70 (/usr/local/lib/python3.11/dist-packages)\n",
            "        nvidia-nvtx-cu12 12.1.105 (/usr/local/lib/python3.11/dist-packages)\n",
            "        sympy 1.13.1 (/usr/local/lib/python3.11/dist-packages)\n",
            "            mpmath 1.3.0 (/usr/local/lib/python3.11/dist-packages)\n",
            "torchaudio 2.5.1+cu121 (/usr/local/lib/python3.11/dist-packages)\n",
            "    torch 2.5.1+cu121 (/usr/local/lib/python3.11/dist-packages)\n",
            "        nvidia-cuda-nvrtc-cu12 12.1.105 (/usr/local/lib/python3.11/dist-packages)\n",
            "        nvidia-cuda-runtime-cu12 12.1.105 (/usr/local/lib/python3.11/dist-packages)\n",
            "        nvidia-cuda-cupti-cu12 12.1.105 (/usr/local/lib/python3.11/dist-packages)\n",
            "        nvidia-cudnn-cu12 9.1.0.70 (/usr/local/lib/python3.11/dist-packages)\n",
            "        nvidia-nvtx-cu12 12.1.105 (/usr/local/lib/python3.11/dist-packages)\n",
            "        sympy 1.13.1 (/usr/local/lib/python3.11/dist-packages)\n",
            "            mpmath 1.3.0 (/usr/local/lib/python3.11/dist-packages)\n",
            "torch 2.5.1+cu121 (/usr/local/lib/python3.11/dist-packages)\n",
            "    nvidia-cuda-nvrtc-cu12 12.1.105 (/usr/local/lib/python3.11/dist-packages)\n",
            "    nvidia-cuda-runtime-cu12 12.1.105 (/usr/local/lib/python3.11/dist-packages)\n",
            "    nvidia-cuda-cupti-cu12 12.1.105 (/usr/local/lib/python3.11/dist-packages)\n",
            "    nvidia-cudnn-cu12 9.1.0.70 (/usr/local/lib/python3.11/dist-packages)\n",
            "    nvidia-nvtx-cu12 12.1.105 (/usr/local/lib/python3.11/dist-packages)\n",
            "    sympy 1.13.1 (/usr/local/lib/python3.11/dist-packages)\n",
            "        mpmath 1.3.0 (/usr/local/lib/python3.11/dist-packages)\n",
            "Found existing installation: mpmath 1.3.0\n",
            "Uninstalling mpmath-1.3.0:\n",
            "  Successfully uninstalled mpmath-1.3.0\n",
            "Found existing installation: torch 2.5.1+cu121\n",
            "Uninstalling torch-2.5.1+cu121:\n",
            "  Successfully uninstalled torch-2.5.1+cu121\n",
            "Found existing installation: torchvision 0.20.1+cu121\n",
            "Uninstalling torchvision-0.20.1+cu121:\n",
            "  Successfully uninstalled torchvision-0.20.1+cu121\n",
            "Found existing installation: sympy 1.13.1\n",
            "Uninstalling sympy-1.13.1:\n",
            "  Successfully uninstalled sympy-1.13.1\n",
            "Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
            "Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
            "  Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
            "Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
            "Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
            "  Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
            "Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
            "Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
            "  Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
            "Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
            "Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
            "  Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
            "Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "  Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "Found existing installation: torchaudio 2.5.1+cu121\n",
            "Uninstalling torchaudio-2.5.1+cu121:\n",
            "  Successfully uninstalled torchaudio-2.5.1+cu121\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (780.5 MB)\n",
            "Collecting torchvision\n",
            "  Using cached https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.3 MB)\n",
            "Collecting torchaudio\n",
            "  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "Requirement already satisfied: xformers in /usr/local/lib/python3.11/dist-packages (0.0.29.post1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Collecting sympy==1.13.1 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
            "  Using cached https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Installing collected packages: mpmath, sympy, nvidia-nvtx-cu12, nvidia-cudnn-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, torch, torchvision, torchaudio\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth-zoo 2025.5.7 requires protobuf<4.0.0, but you have protobuf 5.29.4 which is incompatible.\n",
            "unsloth 2025.5.6 requires protobuf<4.0.0, but you have protobuf 5.29.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mpmath-1.3.0 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-nvtx-cu12-12.1.105 sympy-1.13.1 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121\n",
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.11/dist-packages (2025.5.6)\n",
            "Requirement already satisfied: unsloth_zoo>=2025.5.7 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2025.5.7)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.5.1+cu121)\n",
            "Requirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.0.29.post1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.5)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (24.2)\n",
            "Requirement already satisfied: tyro in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.9.20)\n",
            "Requirement already satisfied: transformers!=4.47.0,==4.51.3 in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.51.3)\n",
            "Requirement already satisfied: datasets>=3.4.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.6.0)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.0.2)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.7.0)\n",
            "Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.15.2)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.15.2)\n",
            "Collecting protobuf<4.0.0 (from unsloth)\n",
            "  Using cached protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.31.2)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.33.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.20.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth) (3.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.1.105)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.4.0->unsloth) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (13.9.4)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.5.7->unsloth) (25.1.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.5.7->unsloth) (11.2.1)\n",
            "Requirement already satisfied: msgspec in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.5.7->unsloth) (0.19.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth) (8.7.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (1.7.2)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth) (2025.4.26)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.19.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (1.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.4.1->unsloth) (1.17.0)\n",
            "Using cached protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "8bd89750f65c4808b73e4ec9c0d9517e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting protobuf<6.0.0,>=5.29.1\n",
            "  Using cached protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Using cached protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth-zoo 2025.5.7 requires protobuf<4.0.0, but you have protobuf 5.29.4 which is incompatible.\n",
            "unsloth 2025.5.6 requires protobuf<4.0.0, but you have protobuf 5.29.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-5.29.4\n",
            "Collecting fsspec==2025.3.2\n",
            "  Using cached fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
            "Using cached fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
            "Installing collected packages: fsspec\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth 2025.5.6 requires protobuf<4.0.0, but you have protobuf 5.29.4 which is incompatible.\n",
            "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fsspec-2025.3.2\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n",
            "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "Installing collected packages: fsspec\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth 2025.5.6 requires protobuf<4.0.0, but you have protobuf 5.29.4 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fsspec-2025.3.0\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.31.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Load the Pre-trained LLaMA Model\n",
        "\n",
        "In this step, we will load the pre-trained LLaMA 3.2 3B model using the `FastLanguageModel` utility from the `unsloth` library. This utility simplifies model loading and supports advanced features like RoPE scaling and 4-bit quantization for memory efficiency."
      ],
      "metadata": {
        "id": "UBVLxgsYMXcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "base_model_name = \"unsloth/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = base_model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "XYIeVNi0MXcs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f92bbafc-c3ef-4592-a5ea-e9553936ec59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.5.6: Fast Llama patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 8.9. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Prepare the Parameter-Efficient Fine-Tuning (PEFT)\n",
        "\n",
        "In this section, we will prepare the pre-trained LLaMA model for fine-tuning using Parameter-Efficient Fine-Tuning (PEFT). PEFT allows us to adapt large language models to new tasks by fine-tuning only a small subset of their parameters, significantly reducing computational and memory requirements.\n",
        "\n",
        "We will leverage the `get_peft_model` function from the `unsloth` library to apply LoRA (Low-Rank Adaptation) fine-tuning, a popular PEFT technique. This involves injecting lightweight, learnable adapters into specific layers of the model. The parameters of the main model remain frozen, making this method both efficient and scalable."
      ],
      "metadata": {
        "id": "5INGM_RSMXct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 42,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "zLNJNJ6yMXct",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "615f8f1d-f99e-4782-b0f8-724289ec3cf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.5.6 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Data Preparation for Supervised Fine-tuning (SFT)\n",
        "We now use the `Llama-3.1` format for conversation style finetunes. We use [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset in ShareGPT style. But we convert it to HuggingFace's normal multiturn format `(\"role\", \"content\")` instead of `(\"from\", \"value\")`/ Llama-3 renders multi turn conversations like below:\n",
        "\n",
        "```\n",
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Hello!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "Hey there! How are you?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "I'm great thanks!<|eot_id|>\n",
        "```\n",
        "\n",
        "The `get_chat_template` function is used to get the correct chat template."
      ],
      "metadata": {
        "id": "gdxt82roMXcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "3ZpKZ2xTMXcu"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now use `standardize_sharegpt` to convert ShareGPT style datasets into HuggingFace's generic format. This changes the dataset from looking like:\n",
        "```\n",
        "{\"from\": \"system\", \"value\": \"You are an assistant\"}\n",
        "{\"from\": \"human\", \"value\": \"What is 2+2?\"}\n",
        "{\"from\": \"gpt\", \"value\": \"It's 4.\"}\n",
        "```\n",
        "to\n",
        "```\n",
        "{\"role\": \"system\", \"content\": \"You are an assistant\"}\n",
        "{\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
        "{\"role\": \"assistant\", \"content\": \"It's 4.\"}\n",
        "```"
      ],
      "metadata": {
        "id": "c0mDuGtEMXcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "dataset = standardize_sharegpt(dataset)\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ],
      "metadata": {
        "trusted": true,
        "id": "5PQqePgJMXcw"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": [
        "We look at how the conversations are structured for an item, e.g., 12:"
      ],
      "metadata": {
        "id": "goKPHDx_MXcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[12][\"conversations\"]"
      ],
      "metadata": {
        "trusted": true,
        "id": "spkxW-LRMXcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f83a1eb-f11f-4b56-a900-9aecbc49d25e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'content': 'Write Python code to solve the task:\\nIn programming, hexadecimal notation is often used.\\n\\nIn hexadecimal notation, besides the ten digits 0, 1, ..., 9, the six letters `A`, `B`, `C`, `D`, `E` and `F` are used to represent the values 10, 11, 12, 13, 14 and 15, respectively.\\n\\nIn this problem, you are given two letters X and Y. Each X and Y is `A`, `B`, `C`, `D`, `E` or `F`.\\n\\nWhen X and Y are seen as hexadecimal numbers, which is larger?\\n\\nConstraints\\n\\n* Each X and Y is `A`, `B`, `C`, `D`, `E` or `F`.\\n\\nInput\\n\\nInput is given from Standard Input in the following format:\\n\\n\\nX Y\\n\\n\\nOutput\\n\\nIf X is smaller, print `<`; if Y is smaller, print `>`; if they are equal, print `=`.\\n\\nExamples\\n\\nInput\\n\\nA B\\n\\n\\nOutput\\n\\n<\\n\\n\\nInput\\n\\nE C\\n\\n\\nOutput\\n\\n>\\n\\n\\nInput\\n\\nF F\\n\\n\\nOutput\\n\\n=',\n",
              "  'role': 'user'},\n",
              " {'content': \"Step 1:  The problem involves comparing two letters in hexadecimal notation.\\nStep 2:  We need to convert the letters X and Y to decimal numbers and then compare them to determine which is larger.\\nStep 3:  We can define a function to convert a letter to its decimal equivalent, and then use this function to compare X and Y.\\nStep 4:  Based on the comparison, we will print '<', '>', or '=' to indicate which letter is larger or if they are equal.\\n```python\\n# Step 5:  Define a function to compare two letters X and Y when they are seen as hexadecimal numbers.\\ndef compare_hex_letters(X, Y):\\n    # Step 6:  Convert the letters X and Y to decimal numbers using the ord function.\\n    \\n    a = ord(X)\\n    b = ord(Y)\\n    # Step 7:  Compare the decimal values of X and Y to determine which is larger. If a is larger, print '>'; if b is larger, print '<'; if they are equal, print '='.\\n    \\n    if(a > b):\\n        print('>')\\n    elif(b > a):\\n        print('<')\\n    else:\\n        print('=')\\n\\n# Step 8:  Process the input provided in the format X Y and pass it to the compare_hex_letters function.\\nX, Y = input().split()\\ncompare_hex_letters(X, Y)\\n\\n```\",\n",
              "  'role': 'assistant'}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we see how the chat template transformed these conversations."
      ],
      "metadata": {
        "id": "8_8GcLmvMXcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[12][\"text\"]"
      ],
      "metadata": {
        "trusted": true,
        "id": "WG0hzEGpMXcx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "0a3fdf95-a1d6-4138-e245-8305c7f08092"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWrite Python code to solve the task:\\nIn programming, hexadecimal notation is often used.\\n\\nIn hexadecimal notation, besides the ten digits 0, 1, ..., 9, the six letters `A`, `B`, `C`, `D`, `E` and `F` are used to represent the values 10, 11, 12, 13, 14 and 15, respectively.\\n\\nIn this problem, you are given two letters X and Y. Each X and Y is `A`, `B`, `C`, `D`, `E` or `F`.\\n\\nWhen X and Y are seen as hexadecimal numbers, which is larger?\\n\\nConstraints\\n\\n* Each X and Y is `A`, `B`, `C`, `D`, `E` or `F`.\\n\\nInput\\n\\nInput is given from Standard Input in the following format:\\n\\n\\nX Y\\n\\n\\nOutput\\n\\nIf X is smaller, print `<`; if Y is smaller, print `>`; if they are equal, print `=`.\\n\\nExamples\\n\\nInput\\n\\nA B\\n\\n\\nOutput\\n\\n<\\n\\n\\nInput\\n\\nE C\\n\\n\\nOutput\\n\\n>\\n\\n\\nInput\\n\\nF F\\n\\n\\nOutput\\n\\n=<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nStep 1:  The problem involves comparing two letters in hexadecimal notation.\\nStep 2:  We need to convert the letters X and Y to decimal numbers and then compare them to determine which is larger.\\nStep 3:  We can define a function to convert a letter to its decimal equivalent, and then use this function to compare X and Y.\\nStep 4:  Based on the comparison, we will print '<', '>', or '=' to indicate which letter is larger or if they are equal.\\n```python\\n# Step 5:  Define a function to compare two letters X and Y when they are seen as hexadecimal numbers.\\ndef compare_hex_letters(X, Y):\\n    # Step 6:  Convert the letters X and Y to decimal numbers using the ord function.\\n    \\n    a = ord(X)\\n    b = ord(Y)\\n    # Step 7:  Compare the decimal values of X and Y to determine which is larger. If a is larger, print '>'; if b is larger, print '<'; if they are equal, print '='.\\n    \\n    if(a > b):\\n        print('>')\\n    elif(b > a):\\n        print('<')\\n    else:\\n        print('=')\\n\\n# Step 8:  Process the input provided in the format X Y and pass it to the compare_hex_letters function.\\nX, Y = input().split()\\ncompare_hex_letters(X, Y)\\n\\n```<|eot_id|>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split the dataset to training and validation set:"
      ],
      "metadata": {
        "id": "YiYBEc7GMXcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset.select(range(0, int(0.8 * len(dataset))))  # 80% for training\n",
        "eval_dataset = dataset.select(range(int(0.8 * len(dataset)), len(dataset)))  # 20% for evaluation"
      ],
      "metadata": {
        "trusted": true,
        "id": "jrJKmT7oMXcy"
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Fine-tune the model with SFT\n",
        "We will use Huggingface TRL's `SFTTrainer`. More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 100 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
      ],
      "metadata": {
        "id": "eS8mS2MwMXcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 100,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 21,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "WtHprqtDMXcy"
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs."
      ],
      "metadata": {
        "id": "T1qHeNgvMXcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "CITelxAwMXcz"
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "source": [
        "We verify masking is actually done:"
      ],
      "metadata": {
        "id": "pMoLIhZnMXcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(trainer.train_dataset[12][\"input_ids\"])"
      ],
      "metadata": {
        "trusted": true,
        "id": "6HIZBEh8MXcz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "ab6d55e4-8ea9-4b7b-d68f-91b2e704e59e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWrite Python code to solve the task:\\nIn programming, hexadecimal notation is often used.\\n\\nIn hexadecimal notation, besides the ten digits 0, 1,..., 9, the six letters `A`, `B`, `C`, `D`, `E` and `F` are used to represent the values 10, 11, 12, 13, 14 and 15, respectively.\\n\\nIn this problem, you are given two letters X and Y. Each X and Y is `A`, `B`, `C`, `D`, `E` or `F`.\\n\\nWhen X and Y are seen as hexadecimal numbers, which is larger?\\n\\nConstraints\\n\\n* Each X and Y is `A`, `B`, `C`, `D`, `E` or `F`.\\n\\nInput\\n\\nInput is given from Standard Input in the following format:\\n\\n\\nX Y\\n\\n\\nOutput\\n\\nIf X is smaller, print `<`; if Y is smaller, print `>`; if they are equal, print `=`.\\n\\nExamples\\n\\nInput\\n\\nA B\\n\\n\\nOutput\\n\\n<\\n\\n\\nInput\\n\\nE C\\n\\n\\nOutput\\n\\n>\\n\\n\\nInput\\n\\nF F\\n\\n\\nOutput\\n\\n=<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nStep 1:  The problem involves comparing two letters in hexadecimal notation.\\nStep 2:  We need to convert the letters X and Y to decimal numbers and then compare them to determine which is larger.\\nStep 3:  We can define a function to convert a letter to its decimal equivalent, and then use this function to compare X and Y.\\nStep 4:  Based on the comparison, we will print '<', '>', or '=' to indicate which letter is larger or if they are equal.\\n```python\\n# Step 5:  Define a function to compare two letters X and Y when they are seen as hexadecimal numbers.\\ndef compare_hex_letters(X, Y):\\n    # Step 6:  Convert the letters X and Y to decimal numbers using the ord function.\\n    \\n    a = ord(X)\\n    b = ord(Y)\\n    # Step 7:  Compare the decimal values of X and Y to determine which is larger. If a is larger, print '>'; if b is larger, print '<'; if they are equal, print '='.\\n    \\n    if(a > b):\\n        print('>')\\n    elif(b > a):\\n        print('<')\\n    else:\\n        print('=')\\n\\n# Step 8:  Process the input provided in the format X Y and pass it to the compare_hex_letters function.\\nX, Y = input().split()\\ncompare_hex_letters(X, Y)\\n\\n```<|eot_id|>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": [
        "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
        "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[12][\"labels\"]])"
      ],
      "metadata": {
        "trusted": true,
        "id": "ubNdLHV2MXcz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "3b76e23d-156d-4402-a5b7-51e27bd08e5b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"                                                                                                                                                                                                                                                                            Step 1:  The problem involves comparing two letters in hexadecimal notation.\\nStep 2:  We need to convert the letters X and Y to decimal numbers and then compare them to determine which is larger.\\nStep 3:  We can define a function to convert a letter to its decimal equivalent, and then use this function to compare X and Y.\\nStep 4:  Based on the comparison, we will print '<', '>', or '=' to indicate which letter is larger or if they are equal.\\n```python\\n# Step 5:  Define a function to compare two letters X and Y when they are seen as hexadecimal numbers.\\ndef compare_hex_letters(X, Y):\\n    # Step 6:  Convert the letters X and Y to decimal numbers using the ord function.\\n    \\n    a = ord(X)\\n    b = ord(Y)\\n    # Step 7:  Compare the decimal values of X and Y to determine which is larger. If a is larger, print '>'; if b is larger, print '<'; if they are equal, print '='.\\n    \\n    if(a > b):\\n        print('>')\\n    elif(b > a):\\n        print('<')\\n    else:\\n        print('=')\\n\\n# Step 8:  Process the input provided in the format X Y and pass it to the compare_hex_letters function.\\nX, Y = input().split()\\ncompare_hex_letters(X, Y)\\n\\n```<|eot_id|>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the System and Instruction prompts are successfully masked."
      ],
      "metadata": {
        "id": "eIgHOTTnMXc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Report current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "f9E-nBQJMXc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "219e9029-1b80-47ac-829c-1b5f873d1920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = NVIDIA L4. Max memory = 22.161 GB.\n",
            "3.441 GB of memory reserved.\n"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5a: Training Progress Monitoring, Checkpoint Management and Early Stopping\n",
        "\n",
        "Before calling `trainer.train()` we define a custom callback to (i) **log metrics** during the training and (ii) manage how we save checkpoints at regular intervals and (iii) implement **early stopping** using the validation set:"
      ],
      "metadata": {
        "id": "m_h1w0E2MXc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "import shutil\n",
        "from transformers import TrainerCallback\n",
        "\n",
        "# Define a custom callback\n",
        "class MetricsLoggerEvalCallback(TrainerCallback):\n",
        "    def __init__(self, log_file, trainer, eval_log_step=10, max_eval_steps=5, patience=3, max_checkpoints=2):\n",
        "        super().__init__()\n",
        "        self.log_file = log_file\n",
        "        self.trainer = trainer\n",
        "        self.eval_log_step = eval_log_step\n",
        "        self.max_eval_steps = max_eval_steps\n",
        "        self.patience = patience\n",
        "        self.max_checkpoints = max_checkpoints\n",
        "        self.best_val_loss = float(\"inf\")\n",
        "        self.no_improve_counter = 0\n",
        "        self.saved_checkpoints = []\n",
        "        self.checkpoint_dir = os.path.join(\"outputs\", \"checkpoints\")\n",
        "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
        "\n",
        "        # Prepare CSV for logging\n",
        "        with open(self.log_file, mode=\"w\", newline=\"\") as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\"Step\", \"Training Loss\", \"Validation Loss\", \"Learning Rate\", \"Memory Usage\"])\n",
        "\n",
        "    def limited_evaluate(self):\n",
        "        \"\"\"\n",
        "        Perform a limited evaluation and return only the validation loss.\n",
        "        \"\"\"\n",
        "        eval_dataloader = self.trainer.get_eval_dataloader()\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for step, batch in enumerate(eval_dataloader):\n",
        "            if step >= self.max_eval_steps:\n",
        "                break\n",
        "\n",
        "            # Perform a prediction step\n",
        "            outputs = trainer.prediction_step(\n",
        "                trainer.model, batch, prediction_loss_only=True\n",
        "            )\n",
        "\n",
        "            # Extract the loss (first item in outputs)\n",
        "            loss = outputs[0]\n",
        "            total_loss += loss.item()  # Convert loss tensor to float\n",
        "            num_batches += 1\n",
        "\n",
        "        # Compute average validation loss\n",
        "        avg_val_loss = total_loss / num_batches if num_batches > 0 else float(\"inf\")\n",
        "        return avg_val_loss\n",
        "\n",
        "    def on_step_end(self, args, state, control, logs=None, **kwargs):\n",
        "        step = state.global_step\n",
        "        if step % self.eval_log_step == 0:\n",
        "            logs = logs or {}\n",
        "            training_loss = logs.get(\"loss\") or (state.log_history[-1].get(\"loss\") if state.log_history else None)\n",
        "            learning_rate = logs.get(\"learning_rate\") or self.trainer.lr_scheduler.get_last_lr()[0]\n",
        "            memory_usage = round(torch.cuda.memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "            val_loss = self.limited_evaluate()\n",
        "\n",
        "            # Log to console\n",
        "            print(f\"Step {step}: Loss={training_loss}, Validation Loss={val_loss}, LR={learning_rate}, Memory={memory_usage} GB\")\n",
        "\n",
        "            # Save to CSV\n",
        "            with open(self.log_file, mode=\"a\", newline=\"\") as file:\n",
        "                writer = csv.writer(file)\n",
        "                writer.writerow([step, training_loss, val_loss, learning_rate, memory_usage])\n",
        "\n",
        "            # Save checkpoint if validation loss improves\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.no_improve_counter = 0\n",
        "                checkpoint_path = os.path.join(self.checkpoint_dir, f\"checkpoint-step-{step}\")\n",
        "                self.trainer.save_model(checkpoint_path)\n",
        "                print(f\"Saved checkpoint at {checkpoint_path}\")\n",
        "\n",
        "                # Manage checkpoint storage\n",
        "                self.saved_checkpoints.append(checkpoint_path)\n",
        "                if len(self.saved_checkpoints) > self.max_checkpoints:\n",
        "                    old_checkpoint = self.saved_checkpoints.pop(0)\n",
        "                    shutil.rmtree(old_checkpoint)\n",
        "                    print(f\"Deleted old checkpoint: {old_checkpoint}\")\n",
        "            else:\n",
        "                self.no_improve_counter += 1\n",
        "                print(f\"No improvement in validation loss for {self.no_improve_counter} evaluations.\")\n",
        "\n",
        "            # Early stopping\n",
        "            if self.no_improve_counter >= self.patience:\n",
        "                print(\"Early stopping triggered. Training terminated.\")\n",
        "                control.should_training_stop = True\n",
        "\n",
        "# Initialize callback\n",
        "log_file = \"outputs/training_metrics.csv\"\n",
        "metrics_logger_evaluator = MetricsLoggerEvalCallback(log_file=log_file, trainer=trainer, eval_log_step=10, max_eval_steps=5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "TjRtam69MXc0"
      },
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.add_callback(metrics_logger_evaluator)\n",
        "\n",
        "# Run training\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "trusted": true,
        "id": "CyyLzdalMXc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "16f659d4-82e3-418c-9e4f-586d600930b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 80,000 | Num Epochs = 1 | Total steps = 100\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 24,313,856/3,000,000,000 (0.81% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 07:03, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.815600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.061100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.086100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.881000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.703400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.103700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.814800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.954500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.857100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.698900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.895400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.997800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.821800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.083500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.799700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.781200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.701800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.821700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.506100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.882800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.713800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.363400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.889300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.955100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.733100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.663700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.068600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.739500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.801100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.775400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.668700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.795100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.888800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.611000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.787300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.689300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.855000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.772600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.741900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.638500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.658300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.790000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.917500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.755900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.662600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.727200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.007600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.097600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.945800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.710500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.603900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.698300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.845600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.165300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.737200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.842600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.125200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.979100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.374000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.752300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.881300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.533400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.736400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.993200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.841400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.682100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.803100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.957400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.800100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.617000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.857600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>1.012700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>1.034600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.662200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>1.015000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.529800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.609900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.710400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.437100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.940400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.697000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.599100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.739600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.998700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.900600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.784500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.697100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.730700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.828400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.911500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.587900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.620100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.647200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.595400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.773500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.760800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.736000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.638200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.772800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.603400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
            "Using gradient accumulation will be very slightly less accurate.\n",
            "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 10: Loss=0.8571, Validation Loss=0.8045812964439392, LR=0.00018947368421052632, Memory=3.422 GB\n",
            "Saved checkpoint at outputs/checkpoints/checkpoint-step-10\n",
            "Step 20: Loss=0.5061, Validation Loss=0.7816206455230713, LR=0.00016842105263157895, Memory=3.969 GB\n",
            "Saved checkpoint at outputs/checkpoints/checkpoint-step-20\n",
            "Step 30: Loss=0.8011, Validation Loss=0.7719449877738953, LR=0.00014736842105263158, Memory=3.969 GB\n",
            "Saved checkpoint at outputs/checkpoints/checkpoint-step-30\n",
            "Deleted old checkpoint: outputs/checkpoints/checkpoint-step-10\n",
            "Step 40: Loss=0.7419, Validation Loss=0.7661083936691284, LR=0.0001263157894736842, Memory=3.971 GB\n",
            "Saved checkpoint at outputs/checkpoints/checkpoint-step-40\n",
            "Deleted old checkpoint: outputs/checkpoints/checkpoint-step-20\n",
            "Step 50: Loss=0.9458, Validation Loss=0.7625956773757935, LR=0.00010526315789473685, Memory=3.971 GB\n",
            "Saved checkpoint at outputs/checkpoints/checkpoint-step-50\n",
            "Deleted old checkpoint: outputs/checkpoints/checkpoint-step-30\n",
            "Step 60: Loss=0.374, Validation Loss=0.7604570031166077, LR=8.421052631578948e-05, Memory=3.971 GB\n",
            "Saved checkpoint at outputs/checkpoints/checkpoint-step-60\n",
            "Deleted old checkpoint: outputs/checkpoints/checkpoint-step-40\n",
            "Step 70: Loss=0.8001, Validation Loss=0.7576814293861389, LR=6.31578947368421e-05, Memory=3.971 GB\n",
            "Saved checkpoint at outputs/checkpoints/checkpoint-step-70\n",
            "Deleted old checkpoint: outputs/checkpoints/checkpoint-step-50\n",
            "Step 80: Loss=0.4371, Validation Loss=0.7571467876434326, LR=4.210526315789474e-05, Memory=4.031 GB\n",
            "Saved checkpoint at outputs/checkpoints/checkpoint-step-80\n",
            "Deleted old checkpoint: outputs/checkpoints/checkpoint-step-60\n",
            "Step 90: Loss=0.8284, Validation Loss=0.756781816482544, LR=2.105263157894737e-05, Memory=4.031 GB\n",
            "Saved checkpoint at outputs/checkpoints/checkpoint-step-90\n",
            "Deleted old checkpoint: outputs/checkpoints/checkpoint-step-70\n",
            "Step 100: Loss=0.7728, Validation Loss=0.7563130736351014, LR=0.0, Memory=4.07 GB\n",
            "Saved checkpoint at outputs/checkpoints/checkpoint-step-100\n",
            "Deleted old checkpoint: outputs/checkpoints/checkpoint-step-80\n"
          ]
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": [
        "# Report final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "aWlra7czMXc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "787e2404-48fb-4d5b-acd3-fd683d650f45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "430.5812 seconds used for training.\n",
            "7.18 minutes used for training.\n",
            "Peak reserved memory = 4.07 GB.\n",
            "Peak reserved memory for training = 0.629 GB.\n",
            "Peak reserved memory % of max memory = 18.366 %.\n",
            "Peak reserved memory for training % of max memory = 2.838 %.\n"
          ]
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5b: Plot key metrics during SFT\n",
        "\n",
        "We need to plot Training Loss, Validation Loss, Learning Rate and Memory Usage.\n",
        "\n"
      ],
      "metadata": {
        "id": "bhIHS-O4MXc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv(\"outputs/training_metrics.csv\")\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
        "fig.suptitle(\"Metrics at Supervised Fine-Tuning\", fontsize = 16)\n",
        "\n",
        "# Training Loss\n",
        "axs[0, 0].plot(df[\"Step\"], df[\"Training Loss\"], label=\"Training Loss\", color=\"blue\")\n",
        "axs[0, 0].set_title(\"Training Loss\")\n",
        "axs[0, 0].set_xlabel(\"Step\")\n",
        "axs[0, 0].set_ylabel(\"Loss\")\n",
        "axs[0, 0].grid(True)\n",
        "\n",
        "# Validation Loss\n",
        "axs[0, 1].plot(df[\"Step\"], df[\"Validation Loss\"], label=\"Validation Loss\", color=\"orange\")\n",
        "axs[0, 1].set_title(\"Validation Loss\")\n",
        "axs[0, 1].set_xlabel(\"Step\")\n",
        "axs[0, 1].set_ylabel(\"Loss\")\n",
        "axs[0, 1].grid(True)\n",
        "\n",
        "# Learning Rate\n",
        "axs[1, 0].plot(df[\"Step\"], df[\"Learning Rate\"], label=\"Learning Rate\", color=\"green\")\n",
        "axs[1, 0].set_title(\"Learning Rate\")\n",
        "axs[1, 0].set_xlabel(\"Step\")\n",
        "axs[1, 0].set_ylabel(\"LR\")\n",
        "axs[1, 0].grid(True)\n",
        "\n",
        "# Memory Usage\n",
        "axs[1, 1].plot(df[\"Step\"], df[\"Memory Usage\"], label=\"Memory (GB)\", color=\"red\")\n",
        "axs[1, 1].set_title(\"GPU Memory Usage\")\n",
        "axs[1, 1].set_xlabel(\"Step\")\n",
        "axs[1, 1].set_ylabel(\"Memory (GB)\")\n",
        "axs[1, 1].grid(True)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iPE4y5nn1aS7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "outputId": "e41734f6-49f0-4482-ebf6-af8340cbede4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x1000 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Run inference on the fine-tuned model\n",
        "Let's run the model. You can specify the instruction.\n",
        "\n",
        "We use `min_p = 0.1` and `temperature = 1.5`. Read this [Tweet](https://x.com/menhguin/status/1826132708508213629) for more information on why."
      ],
      "metadata": {
        "id": "OGr3P27iMXc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
        "                         temperature = 1.5, min_p = 0.1)\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "trusted": true,
        "id": "knG7iP4cMXc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fcffa0f-f968-4a18-89d3-e8e253a27309"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nContinue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nSure, here is a continuation of the Fibonacci sequence starting from where it was left off:\\n\\n1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144\\n\\nThe Fibonacci sequence is a series of numbers in which each']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "execution_count": 18
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time."
      ],
      "metadata": {
        "id": "ncLg1APfMXc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
        "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "OxPfBlEQMXc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f74d678-1b8f-4c07-c1ce-71b2a693ee1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Fibonacci sequence is a series of numbers in which each number is the sum of the two preceding numbers, starting from 1 and 1. Therefore, after 8, the next numbers in the sequence would be 13 and 21.\n",
            "\n",
            "So, the continued Fibonacci sequence would be: 1, 1, 2, 3, 5, 8, 13, 21.<|eot_id|>\n"
          ]
        }
      ],
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Generate Responses Using a Helper Function\n",
        "Now, we will define a helper function to make the model generate responses based on a given instruction. This function simplifies the inference process and can be used to test the model's behavior after fine-tuning."
      ],
      "metadata": {
        "id": "Dc0lOpJHMXc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(instruction, model, tokenizer, max_new_tokens=128):\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": instruction},\n",
        "    ]\n",
        "\n",
        "    # Tokenize the input instruction\n",
        "    inputs = tokenizer(instruction, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate response using the model\n",
        "    outputs = model.generate(\n",
        "        # Dictionary unpacking\n",
        "        **inputs,\n",
        "        max_new_tokens = max_new_tokens,\n",
        "        do_sample = True,\n",
        "        # Τemperature ψontrols the randomness of the sampling process\n",
        "        # by scaling the logits before applying softmax\n",
        "        temperature = 0.7,\n",
        "        top_p = 0.9\n",
        "    )\n",
        "\n",
        "    # Decode and return the response\n",
        "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]"
      ],
      "metadata": {
        "trusted": true,
        "id": "GHvYqrLwMXc2"
      },
      "outputs": [],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "instruction = \"Explain the significance of LoRA in fine-tuning large language models.\"\n",
        "response = generate_response(instruction, model, tokenizer, max_new_tokens=512)\n",
        "print(f\"Instruction: {instruction}\\nResponse: {response}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "3zrUryRiMXc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fab17cb0-78d4-4c93-fa88-de216eb1a342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction: Explain the significance of LoRA in fine-tuning large language models.\n",
            "Response: Explain the significance of LoRA in fine-tuning large language models. \n",
            "### Step 1: Understand the concept of LoRA\n",
            "LoRA (Layer-wise Regularization) is a technique used to fine-tune large language models by selectively pruning layers and updating the remaining layers. This technique helps in reducing the computational cost and memory requirements of the model.\n",
            "\n",
            "### Step 2: Explain the significance of LoRA in fine-tuning large language models\n",
            "LoRA is significant because it allows for more efficient and cost-effective fine-tuning of large language models. By selectively pruning layers, LoRA reduces the computational cost and memory requirements of the model, making it more feasible to fine-tune large models on smaller datasets. Additionally, LoRA helps in reducing the overfitting of the model by pruning layers that are not contributing significantly to the model's performance. This results in a more accurate and efficient model that can be fine-tuned effectively. \n",
            "\n",
            "### Step 3: Summarize the significance of LoRA in fine-tuning large language models\n",
            "In summary, LoRA is a significant technique in fine-tuning large language models because it allows for more efficient and cost-effective fine-tuning by selectively pruning layers and reducing the computational cost and memory requirements of the model. Additionally, LoRA helps in reducing the overfitting of the model by pruning layers that are not contributing significantly to the model's performance. This results in a more accurate and efficient model that can be fine-tuned effectively. \n",
            "\n",
            "### Step 4: Provide a conclusion on the significance of LoRA in fine-tuning large language models\n",
            "In conclusion, LoRA is a significant technique in fine-tuning large language models because it allows for more efficient and cost-effective fine-tuning by selectively pruning layers and reducing the computational cost and memory requirements of the model. Additionally, LoRA helps in reducing the overfitting of the model by pruning layers that are not contributing significantly to the model's performance. This results in a more accurate and efficient model that can be fine-tuned effectively. \n",
            "\n",
            "```python\n",
            "# No code is required for this task.\n",
            "```\n"
          ]
        }
      ],
      "execution_count": 21
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Saving, loading fine-tuned models\n",
        "To save the final model as LoRA adapters, we use `save_pretrained` for a local save.\n",
        "\n",
        "**Note** that this ONLY saves the LoRA adapters, and not the full model."
      ],
      "metadata": {
        "id": "NorlnOScMXc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "MT9_YknWMXc-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35a0b9d0-e277-4321-e7fe-2ae171ebc4be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "execution_count": 22
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the model we just saved:"
      ],
      "metadata": {
        "id": "PaHqoX1bMXc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sft_model, sft_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"lora_model\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "FastLanguageModel.for_inference(sft_model) # Enable native 2x faster inference\n",
        "\n",
        "# run inference\n",
        "instruction = \"Describe a tall tower in the capital of France.\"\n",
        "response = generate_response(instruction, sft_model, sft_tokenizer, max_new_tokens=512)\n",
        "print(f\"Instruction: {instruction}\\nResponse: {response}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "06dHwEmuMXc_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3588f8d9-dd9c-4ab5-9b61-1702bd91fbbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.5.6: Fast Llama patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 8.9. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Instruction: Describe a tall tower in the capital of France.\n",
            "Response: Describe a tall tower in the capital of France. The tower is known for its unique architecture and is considered a symbol of the country's rich history and culture. The tower is built on a small island in the city, and its walls are covered in ivy, with windows that reflect the colors of the rainbow. The tower is surrounded by a beautiful garden filled with vibrant flowers and the soothing sounds of a small fountain.\n",
            "\n",
            "The tower stands at an impressive height of 50 meters, and its design is a perfect blend of medieval and modern styles. The walls are made of a unique blend of limestone and granite, giving the tower a distinctive appearance that is both sturdy and elegant. The tower's unique architecture has made it a popular destination for tourists and locals alike, who come to marvel at its beauty and learn about its rich history.\n",
            "\n",
            "The tower is home to a museum that showcases the country's rich cultural heritage, with exhibits that range from ancient artifacts to modern works of art. The museum is designed to be interactive, with hands-on exhibits that allow visitors to explore the country's history and culture in a fun and engaging way. The museum's collection includes a wide range of artifacts, from ancient coins and pottery to modern paintings and sculptures.\n",
            "\n",
            "The tower's history is steeped in tradition, and it has been a part of the city's landscape for centuries. The tower was built in the 16th century, and it has been restored and renovated several times over the years. The tower's design has been influenced by various architectural styles, including Gothic, Renaissance, and Baroque.\n",
            "\n",
            "The tower is a popular destination for tourists and locals alike, who come to marvel at its beauty and learn about its rich history. The tower's unique architecture and stunning design have made it a symbol of France's rich cultural heritage, and it is considered one of the country's most iconic landmarks. The tower is a must-visit destination for anyone interested in history, architecture, and culture, and it is sure to leave a lasting impression on all who visit.\n"
          ]
        }
      ],
      "execution_count": 23
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Evaluate Models using OpenAssistant Conversations Dataset\n",
        "\n",
        "To evaluate the performance of the base model and LoRA-adapted model, we will use the [OpenAssistant Conversations Dataset](https://huggingface.co/datasets/OpenAssistant/oasst1). This dataset provides instruction-response pairs in a conversational format, allowing us to test the model's ability to follow instructions and generate coherent, contextually relevant responses."
      ],
      "metadata": {
        "id": "Zf4UkriuMXc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare the evaluation dataset\n",
        "The dataset uses the roles `prompter` and `assistant`. Prompter messages (role: \"prompter\") can be considered instructions, and assistant messages (role: \"assistant\") as corresponding responses.\n",
        "\n",
        "We construct input-output pairs by traversing the message tree (`parent_id` field) and linking each prompter message to its next assistant message. We will use a part of the dataset, e.g., 100 samples."
      ],
      "metadata": {
        "id": "gngzOtxeMXc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"OpenAssistant/oasst1\", split=\"validation\")\n",
        "\n",
        "# Extract input-output pairs\n",
        "def extract_pairs(dataset, n_samples=100):\n",
        "    pairs = []\n",
        "    for sample in dataset:\n",
        "        if sample[\"lang\"] != \"en\":\n",
        "            continue\n",
        "        if sample[\"role\"] == \"prompter\":\n",
        "            # Find the child message with 'assistant' role\n",
        "            child_message = next(\n",
        "                (item for item in dataset if item[\"parent_id\"] == sample[\"message_id\"] and item[\"role\"] == \"assistant\"),\n",
        "                None\n",
        "            )\n",
        "            if child_message:\n",
        "                pairs.append({\n",
        "                    \"instruction\": sample[\"text\"],\n",
        "                    \"response\": child_message[\"text\"]\n",
        "                })\n",
        "        if len(pairs) == n_samples:\n",
        "            break\n",
        "    return pairs\n",
        "\n",
        "input_output_pairs = extract_pairs(dataset)\n",
        "\n",
        "# Inspect extracted pairs\n",
        "print(f\"Extracted {len(input_output_pairs)} input-output pairs.\")\n",
        "print(input_output_pairs[:3])"
      ],
      "metadata": {
        "trusted": true,
        "id": "u77Njvv5MXc_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fb360c4-1c31-46e5-941d-304303cb98cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 100 input-output pairs.\n",
            "[{'instruction': 'What do you think about ChatGPT?', 'response': \"As an open source alternative to ChatGPT, I do not have personal opinions. However, I can provide objective information about ChatGPT's capabilities and limitations based on its architecture and training data.\\n\\nChatGPT is a powerful language model based on the GPT (Generative Pre-trained Transformer) architecture developed by OpenAI. It has been trained on a large corpus of text data and is capable of generating human-like responses to a wide variety of prompts and questions. ChatGPT's ability to generate coherent and contextually relevant responses is based on its ability to understand the structure of language and make predictions about what words and phrases are likely to follow in a given context.\\n\\nHowever, like all language models, ChatGPT has limitations. Its responses are based solely on the patterns it has learned from its training data and it may generate responses that are biased, incomplete, or inaccurate based on the input it receives. Additionally, ChatGPT may struggle with tasks that require a deep understanding of context, such as detecting sarcasm or understanding humor.\\n\\nOverall, ChatGPT is a powerful language model that has demonstrated impressive capabilities in generating human-like responses. However, it is important to recognize its limitations and use it appropriately, while continuing to develop and refine its capabilities through ongoing research and development.\"}, {'instruction': \"What are your thoughts on the censorship of ChatGPT's output and its liberal biases?\", 'response': 'As a large language model trained on text from the internet, ChatGPT will have learned its biases according to the opinions of the people who have written the text online. Some of the text it has seen online could potentially contain harmful content that incites violence or aggression towards other people, so ChatGPT is perfectly capable of reproducing these views and insisting on their validity in face of opposition. In order to act as a useful assistant, ChatGPT should be able to distinguish fact from opinion. As it might not be able to do this perfectly, censorship is probably the safest approach OpenAI could take to prevent toxic content from the model output.'}, {'instruction': 'Yeah, I hear you, brother! Power to the people! So how will you be different than ChatGPT?', 'response': 'Here are some differences between me and ChatGPT:\\n\\n- I will be able to interact with third-party systems, such as using APIs and third-party third-party applications.\\n- I will be able to retrieve information dynamically from search engines to better perform tasks.\\n- As a free open-source software, I can be extended and personalized easily.\\n- I will be able to run on consumer hardwares.\\n\\nMore about me can be find in my [Vision & Roadmap](https://docs.google.com/presentation/d/1n7IrAOVOqwdYgiYrXc8Sj0He8krn5MVZO_iLkCjTtu0/edit?usp=sharing).'}]\n"
          ]
        }
      ],
      "execution_count": 24
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper function for getting model results"
      ],
      "metadata": {
        "id": "Xnn-g2EXMXdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "def get_model_results(model, tokenizer, pairs):\n",
        "\n",
        "    results = []\n",
        "    for pair in tqdm(pairs):\n",
        "        # Extract the user instruction from the pair\n",
        "        instruction = pair[\"instruction\"]\n",
        "        # Extract the ground truth response\n",
        "        reference = pair[\"response\"]\n",
        "        prediction = generate_response(instruction, model, tokenizer)\n",
        "\n",
        "        results.append({\n",
        "            \"instruction\": instruction,\n",
        "            \"reference\": reference,\n",
        "            \"prediction\": prediction\n",
        "        })\n",
        "    return results"
      ],
      "metadata": {
        "trusted": true,
        "id": "qsnw9juwMXdA"
      },
      "outputs": [],
      "execution_count": 25
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the results of both pre-trained (base) and SFT models"
      ],
      "metadata": {
        "id": "q1lR-orqMXdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load base model for inference\n",
        "base_model, base_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = base_model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "FastLanguageModel.for_inference(base_model)"
      ],
      "metadata": {
        "trusted": true,
        "id": "O_BwodJmMXdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad9a37f1-82f5-4b75-9bbf-a9205bac4422"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.5.6: Fast Llama patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 8.9. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
              "    (layers): ModuleList(\n",
              "      (0): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
              "      )\n",
              "      (1): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
              "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
              "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
              "      )\n",
              "      (2-27): 26 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the results of both models\n",
        "base_model_results = get_model_results(base_model, tokenizer, input_output_pairs)\n",
        "sft_model_results = get_model_results(sft_model, tokenizer, input_output_pairs)"
      ],
      "metadata": {
        "trusted": true,
        "id": "G9TvJqR5MXdB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "739d9566-b68e-4be2-e58d-59857af30811"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [08:14<00:00,  4.94s/it]\n",
            "100%|██████████| 100/100 [11:06<00:00,  6.66s/it]\n"
          ]
        }
      ],
      "execution_count": 27
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply Evaluation Metrics\n",
        "\n",
        "We can now compute metrics like BLEU, ROUGE, or BERTScore using the reference and prediction fields."
      ],
      "metadata": {
        "id": "kQqVCBwIMXdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "def evaluate_model_results(model_results, metric_name):\n",
        "\n",
        "    metric = evaluate.load(metric_name)\n",
        "    references = [result[\"reference\"] for result in model_results] # Ground-truth responses\n",
        "    predictions = [result[\"prediction\"] for result in model_results] # Model responses\n",
        "\n",
        "    # Compute the metric\n",
        "    if metric_name == \"bertscore\":\n",
        "      evaluation = metric.compute(predictions=predictions,\n",
        "                                  references=[[ref] for ref in references],\n",
        "                                  lang=\"en\")  # Specify the language for BERTScore\n",
        "    else:\n",
        "      evaluation = metric.compute(predictions=predictions,\n",
        "                                  references=[[ref] for ref in references])\n",
        "\n",
        "    return evaluation"
      ],
      "metadata": {
        "trusted": true,
        "id": "8_Iv2me8MXdB"
      },
      "outputs": [],
      "execution_count": 28
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score\n",
        "!pip install bert-score"
      ],
      "metadata": {
        "id": "8VbEQK1HAPn1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94fe15ac-3f2e-4641-f176-dc8e3d5e5c3a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Requirement already satisfied: bert-score in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.5.1+cu121)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.51.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->bert-score) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.31.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.4.26)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare models using several evaluation metrics\n",
        "import numpy as np\n",
        "# Evaluate using BLEU\n",
        "bleu_base = evaluate_model_results(base_model_results, \"bleu\")\n",
        "bleu_finetuned = evaluate_model_results(sft_model_results, \"bleu\")\n",
        "print(f\"BLEU Score - Base Model: {bleu_base}\")\n",
        "print(f\"BLEU Score - Fine-tuned Model: {bleu_finetuned}\")\n",
        "\n",
        "# Evaluate using ROUGE\n",
        "rouge_base = evaluate_model_results(base_model_results, \"rouge\")\n",
        "rouge_finetuned = evaluate_model_results(sft_model_results, \"rouge\")\n",
        "print(f\"ROUGE Score - Base Model: {rouge_base}\")\n",
        "print(f\"ROUGE Score - Fine-tuned Model: {rouge_finetuned}\")\n",
        "\n",
        "# Evaluate using BERTScore\n",
        "bertscore_base = evaluate_model_results(base_model_results, \"bertscore\")\n",
        "bertscore_finetuned = evaluate_model_results(sft_model_results, \"bertscore\")\n",
        "print(\"BERTScore - Base Model:\")\n",
        "print(f\"  Precision: {np.mean(bertscore_base['precision']):.4f}\")\n",
        "print(f\"  Recall:    {np.mean(bertscore_base['recall']):.4f}\")\n",
        "print(f\"  F1 Score:  {np.mean(bertscore_base['f1']):.4f}\")\n",
        "\n",
        "print(\"BERTScore - Fine-tuned Model:\")\n",
        "print(f\"  Precision: {np.mean(bertscore_finetuned['precision']):.4f}\")\n",
        "print(f\"  Recall:    {np.mean(bertscore_finetuned['recall']):.4f}\")\n",
        "print(f\"  F1 Score:  {np.mean(bertscore_finetuned['f1']):.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "6KB_uCi_MXdC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36c3bbed-ef25-486d-a582-8b52e24a2f68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score - Base Model: {'bleu': 0.06380060048774792, 'precisions': [0.33880813038827007, 0.10081692103096701, 0.0484991396341852, 0.02822140978769803], 'brevity_penalty': 0.7715689961234542, 'length_ratio': 0.7940735558664801, 'translation_length': 15891, 'reference_length': 20012}\n",
            "BLEU Score - Fine-tuned Model: {'bleu': 0.06920717246816778, 'precisions': [0.34525837592277114, 0.11061019747285541, 0.0537414531279954, 0.0319634703196347], 'brevity_penalty': 0.7689983814041056, 'length_ratio': 0.7919748151109335, 'translation_length': 15849, 'reference_length': 20012}\n",
            "ROUGE Score - Base Model: {'rouge1': np.float64(0.28232659063091153), 'rouge2': np.float64(0.0828013115691005), 'rougeL': np.float64(0.16161558794097278), 'rougeLsum': np.float64(0.23634777100470755)}\n",
            "ROUGE Score - Fine-tuned Model: {'rouge1': np.float64(0.27952943242356876), 'rouge2': np.float64(0.08209215956237667), 'rougeL': np.float64(0.16289714529005545), 'rougeLsum': np.float64(0.2340918944057058)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERTScore - Base Model:\n",
            "  Precision: 0.8397\n",
            "  Recall:    0.8384\n",
            "  F1 Score:  0.8388\n",
            "BERTScore - Fine-tuned Model:\n",
            "  Precision: 0.8415\n",
            "  Recall:    0.8374\n",
            "  F1 Score:  0.8392\n"
          ]
        }
      ],
      "execution_count": 30
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Fine-tune the base model using DPO\n"
      ],
      "metadata": {
        "id": "jx-0phs4MXdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Report current memory stats\n",
        "import torch\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "id": "2ofqqoG-I6JL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c19e6250-6cd7-4aac-a514-6e306a4b36d2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = NVIDIA L4. Max memory = 22.161 GB.\n",
            "11.281 GB of memory reserved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments\n",
        "from trl import DPOTrainer\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from peft import PeftModel\n",
        "import traceback\n",
        "\n",
        "# Copied Step 2 and Step 3 from SFT Training\n",
        "# ------------------------------------------\n",
        "max_seq_length = 2048\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "base_model_name = \"unsloth/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = base_model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 42,\n",
        "    max_seq_length=max_seq_length,\n",
        ")\n",
        "# ------------------------------------------\n",
        "\n",
        "# Sanity checks\n",
        "print(f\"Tokenizer: {tokenizer}\")\n",
        "print(f\"Tokenizer pad_token: {tokenizer.pad_token}, pad_token_id: {tokenizer.pad_token_id}\")\n",
        "if tokenizer.pad_token is None:\n",
        "    print(\"CRITICAL WARNING: tokenizer.pad_token is None.\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Model type: {type(model)}\")\n",
        "\n",
        "current_use_cache = None\n",
        "\n",
        "# ------------------------------------------\n",
        "dpo_dataset_full = load_dataset(\"Intel/orca_dpo_pairs\", split = \"train\")\n",
        "dpo_dataset_full = dpo_dataset_full.rename_column('question', 'prompt')\n",
        "\n",
        "\n",
        "split_dataset = dpo_dataset_full.train_test_split(test_size = 0.1, seed = 42)\n",
        "train_dpo_dataset = split_dataset[\"train\"]\n",
        "eval_dpo_dataset = split_dataset[\"test\"]\n",
        "# ------------------------------------------\n",
        "\n",
        "# Copied Step 5 from SFT Training\n",
        "# ------------------------------------------\n",
        "print(\"Initializing DPOTrainer...\")\n",
        "training_args = TrainingArguments(\n",
        "    # Basic setup\n",
        "    output_dir = \"outputs_dpo_debug\",\n",
        "    seed = 21,\n",
        "    report_to = \"none\",\n",
        "    remove_unused_columns = False,  # Important for DPOTrainer\n",
        "\n",
        "    # Training control\n",
        "    max_steps = 100,\n",
        "    per_device_train_batch_size = 2,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    warmup_steps = 5,\n",
        "\n",
        "    # Optimizer and scheduler\n",
        "    learning_rate = 2e-4,\n",
        "    weight_decay = 0.01,\n",
        "    optim = \"adamw_8bit\",\n",
        "    lr_scheduler_type = \"linear\",\n",
        "\n",
        "    # Precision\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "\n",
        "    # Evaluation & logging\n",
        "    logging_steps = 1,\n",
        ")\n",
        "\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model = model,\n",
        "    ref_model = None,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dpo_dataset,\n",
        "    eval_dataset = eval_dpo_dataset,\n",
        "    args = training_args,\n",
        ")\n",
        "print(\"DPOTrainer initialized.\")\n",
        "# ------------------------------------------\n",
        "# Initialize and add callback\n",
        "# dpo_metrics_log_file = \"outputs/dpo_training_metrics.csv\"\n",
        "# dpo_metrics_logger_evaluator = MetricsLoggerEvalCallback(log_file=dpo_metrics_log_file, trainer=dpo_trainer, eval_log_step=10, max_eval_steps=5)\n",
        "# dpo_trainer.add_callback(dpo_metrics_logger_evaluator)\n",
        "# ------------------------------------------\n",
        "# Training\n",
        "try:\n",
        "    dpo_trainer_stats = dpo_trainer.train()\n",
        "    print(\"DPO training finished.\")\n",
        "    if dpo_trainer_stats:\n",
        "        print(dpo_trainer_stats)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during dpo_trainer.train():\")\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dPcLbiWb0Ee2",
        "outputId": "b0350e22-4091-4c57-bed5-0b4bee83d0ba"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.5.6: Fast Llama patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 8.9. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Tokenizer: PreTrainedTokenizerFast(name_or_path='unsloth/llama-3.2-3b-instruct-unsloth-bnb-4bit', vocab_size=128000, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|begin_of_text|>', 'eos_token': '<|eot_id|>', 'pad_token': '<|finetune_right_pad_id|>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
            "\t128000: AddedToken(\"<|begin_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128001: AddedToken(\"<|end_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128002: AddedToken(\"<|reserved_special_token_0|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128003: AddedToken(\"<|reserved_special_token_1|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128004: AddedToken(\"<|finetune_right_pad_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128005: AddedToken(\"<|reserved_special_token_2|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128006: AddedToken(\"<|start_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128007: AddedToken(\"<|end_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128008: AddedToken(\"<|eom_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128009: AddedToken(\"<|eot_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128010: AddedToken(\"<|python_tag|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128011: AddedToken(\"<|reserved_special_token_3|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128012: AddedToken(\"<|reserved_special_token_4|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128013: AddedToken(\"<|reserved_special_token_5|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128014: AddedToken(\"<|reserved_special_token_6|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128015: AddedToken(\"<|reserved_special_token_7|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128016: AddedToken(\"<|reserved_special_token_8|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128017: AddedToken(\"<|reserved_special_token_9|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128018: AddedToken(\"<|reserved_special_token_10|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128019: AddedToken(\"<|reserved_special_token_11|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128020: AddedToken(\"<|reserved_special_token_12|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128021: AddedToken(\"<|reserved_special_token_13|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128022: AddedToken(\"<|reserved_special_token_14|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128023: AddedToken(\"<|reserved_special_token_15|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128024: AddedToken(\"<|reserved_special_token_16|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128025: AddedToken(\"<|reserved_special_token_17|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128026: AddedToken(\"<|reserved_special_token_18|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128027: AddedToken(\"<|reserved_special_token_19|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128028: AddedToken(\"<|reserved_special_token_20|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128029: AddedToken(\"<|reserved_special_token_21|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128030: AddedToken(\"<|reserved_special_token_22|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128031: AddedToken(\"<|reserved_special_token_23|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128032: AddedToken(\"<|reserved_special_token_24|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128033: AddedToken(\"<|reserved_special_token_25|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128034: AddedToken(\"<|reserved_special_token_26|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128035: AddedToken(\"<|reserved_special_token_27|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128036: AddedToken(\"<|reserved_special_token_28|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128037: AddedToken(\"<|reserved_special_token_29|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128038: AddedToken(\"<|reserved_special_token_30|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128039: AddedToken(\"<|reserved_special_token_31|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128040: AddedToken(\"<|reserved_special_token_32|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128041: AddedToken(\"<|reserved_special_token_33|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128042: AddedToken(\"<|reserved_special_token_34|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128043: AddedToken(\"<|reserved_special_token_35|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128044: AddedToken(\"<|reserved_special_token_36|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128045: AddedToken(\"<|reserved_special_token_37|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128046: AddedToken(\"<|reserved_special_token_38|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128047: AddedToken(\"<|reserved_special_token_39|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128048: AddedToken(\"<|reserved_special_token_40|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128049: AddedToken(\"<|reserved_special_token_41|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128050: AddedToken(\"<|reserved_special_token_42|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128051: AddedToken(\"<|reserved_special_token_43|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128052: AddedToken(\"<|reserved_special_token_44|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128053: AddedToken(\"<|reserved_special_token_45|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128054: AddedToken(\"<|reserved_special_token_46|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128055: AddedToken(\"<|reserved_special_token_47|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128056: AddedToken(\"<|reserved_special_token_48|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128057: AddedToken(\"<|reserved_special_token_49|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128058: AddedToken(\"<|reserved_special_token_50|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128059: AddedToken(\"<|reserved_special_token_51|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128060: AddedToken(\"<|reserved_special_token_52|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128061: AddedToken(\"<|reserved_special_token_53|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128062: AddedToken(\"<|reserved_special_token_54|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128063: AddedToken(\"<|reserved_special_token_55|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128064: AddedToken(\"<|reserved_special_token_56|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128065: AddedToken(\"<|reserved_special_token_57|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128066: AddedToken(\"<|reserved_special_token_58|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128067: AddedToken(\"<|reserved_special_token_59|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128068: AddedToken(\"<|reserved_special_token_60|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128069: AddedToken(\"<|reserved_special_token_61|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128070: AddedToken(\"<|reserved_special_token_62|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128071: AddedToken(\"<|reserved_special_token_63|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128072: AddedToken(\"<|reserved_special_token_64|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128073: AddedToken(\"<|reserved_special_token_65|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128074: AddedToken(\"<|reserved_special_token_66|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128075: AddedToken(\"<|reserved_special_token_67|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128076: AddedToken(\"<|reserved_special_token_68|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128077: AddedToken(\"<|reserved_special_token_69|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128078: AddedToken(\"<|reserved_special_token_70|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128079: AddedToken(\"<|reserved_special_token_71|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128080: AddedToken(\"<|reserved_special_token_72|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128081: AddedToken(\"<|reserved_special_token_73|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128082: AddedToken(\"<|reserved_special_token_74|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128083: AddedToken(\"<|reserved_special_token_75|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128084: AddedToken(\"<|reserved_special_token_76|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128085: AddedToken(\"<|reserved_special_token_77|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128086: AddedToken(\"<|reserved_special_token_78|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128087: AddedToken(\"<|reserved_special_token_79|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128088: AddedToken(\"<|reserved_special_token_80|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128089: AddedToken(\"<|reserved_special_token_81|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128090: AddedToken(\"<|reserved_special_token_82|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128091: AddedToken(\"<|reserved_special_token_83|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128092: AddedToken(\"<|reserved_special_token_84|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128093: AddedToken(\"<|reserved_special_token_85|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128094: AddedToken(\"<|reserved_special_token_86|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128095: AddedToken(\"<|reserved_special_token_87|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128096: AddedToken(\"<|reserved_special_token_88|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128097: AddedToken(\"<|reserved_special_token_89|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128098: AddedToken(\"<|reserved_special_token_90|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128099: AddedToken(\"<|reserved_special_token_91|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128100: AddedToken(\"<|reserved_special_token_92|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128101: AddedToken(\"<|reserved_special_token_93|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128102: AddedToken(\"<|reserved_special_token_94|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128103: AddedToken(\"<|reserved_special_token_95|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128104: AddedToken(\"<|reserved_special_token_96|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128105: AddedToken(\"<|reserved_special_token_97|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128106: AddedToken(\"<|reserved_special_token_98|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128107: AddedToken(\"<|reserved_special_token_99|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128108: AddedToken(\"<|reserved_special_token_100|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128109: AddedToken(\"<|reserved_special_token_101|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128110: AddedToken(\"<|reserved_special_token_102|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128111: AddedToken(\"<|reserved_special_token_103|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128112: AddedToken(\"<|reserved_special_token_104|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128113: AddedToken(\"<|reserved_special_token_105|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128114: AddedToken(\"<|reserved_special_token_106|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128115: AddedToken(\"<|reserved_special_token_107|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128116: AddedToken(\"<|reserved_special_token_108|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128117: AddedToken(\"<|reserved_special_token_109|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128118: AddedToken(\"<|reserved_special_token_110|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128119: AddedToken(\"<|reserved_special_token_111|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128120: AddedToken(\"<|reserved_special_token_112|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128121: AddedToken(\"<|reserved_special_token_113|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128122: AddedToken(\"<|reserved_special_token_114|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128123: AddedToken(\"<|reserved_special_token_115|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128124: AddedToken(\"<|reserved_special_token_116|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128125: AddedToken(\"<|reserved_special_token_117|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128126: AddedToken(\"<|reserved_special_token_118|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128127: AddedToken(\"<|reserved_special_token_119|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128128: AddedToken(\"<|reserved_special_token_120|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128129: AddedToken(\"<|reserved_special_token_121|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128130: AddedToken(\"<|reserved_special_token_122|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128131: AddedToken(\"<|reserved_special_token_123|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128132: AddedToken(\"<|reserved_special_token_124|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128133: AddedToken(\"<|reserved_special_token_125|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128134: AddedToken(\"<|reserved_special_token_126|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128135: AddedToken(\"<|reserved_special_token_127|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128136: AddedToken(\"<|reserved_special_token_128|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128137: AddedToken(\"<|reserved_special_token_129|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128138: AddedToken(\"<|reserved_special_token_130|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128139: AddedToken(\"<|reserved_special_token_131|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128140: AddedToken(\"<|reserved_special_token_132|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128141: AddedToken(\"<|reserved_special_token_133|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128142: AddedToken(\"<|reserved_special_token_134|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128143: AddedToken(\"<|reserved_special_token_135|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128144: AddedToken(\"<|reserved_special_token_136|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128145: AddedToken(\"<|reserved_special_token_137|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128146: AddedToken(\"<|reserved_special_token_138|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128147: AddedToken(\"<|reserved_special_token_139|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128148: AddedToken(\"<|reserved_special_token_140|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128149: AddedToken(\"<|reserved_special_token_141|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128150: AddedToken(\"<|reserved_special_token_142|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128151: AddedToken(\"<|reserved_special_token_143|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128152: AddedToken(\"<|reserved_special_token_144|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128153: AddedToken(\"<|reserved_special_token_145|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128154: AddedToken(\"<|reserved_special_token_146|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128155: AddedToken(\"<|reserved_special_token_147|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128156: AddedToken(\"<|reserved_special_token_148|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128157: AddedToken(\"<|reserved_special_token_149|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128158: AddedToken(\"<|reserved_special_token_150|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128159: AddedToken(\"<|reserved_special_token_151|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128160: AddedToken(\"<|reserved_special_token_152|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128161: AddedToken(\"<|reserved_special_token_153|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128162: AddedToken(\"<|reserved_special_token_154|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128163: AddedToken(\"<|reserved_special_token_155|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128164: AddedToken(\"<|reserved_special_token_156|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128165: AddedToken(\"<|reserved_special_token_157|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128166: AddedToken(\"<|reserved_special_token_158|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128167: AddedToken(\"<|reserved_special_token_159|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128168: AddedToken(\"<|reserved_special_token_160|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128169: AddedToken(\"<|reserved_special_token_161|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128170: AddedToken(\"<|reserved_special_token_162|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128171: AddedToken(\"<|reserved_special_token_163|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128172: AddedToken(\"<|reserved_special_token_164|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128173: AddedToken(\"<|reserved_special_token_165|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128174: AddedToken(\"<|reserved_special_token_166|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128175: AddedToken(\"<|reserved_special_token_167|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128176: AddedToken(\"<|reserved_special_token_168|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128177: AddedToken(\"<|reserved_special_token_169|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128178: AddedToken(\"<|reserved_special_token_170|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128179: AddedToken(\"<|reserved_special_token_171|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128180: AddedToken(\"<|reserved_special_token_172|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128181: AddedToken(\"<|reserved_special_token_173|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128182: AddedToken(\"<|reserved_special_token_174|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128183: AddedToken(\"<|reserved_special_token_175|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128184: AddedToken(\"<|reserved_special_token_176|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128185: AddedToken(\"<|reserved_special_token_177|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128186: AddedToken(\"<|reserved_special_token_178|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128187: AddedToken(\"<|reserved_special_token_179|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128188: AddedToken(\"<|reserved_special_token_180|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128189: AddedToken(\"<|reserved_special_token_181|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128190: AddedToken(\"<|reserved_special_token_182|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128191: AddedToken(\"<|reserved_special_token_183|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128192: AddedToken(\"<|reserved_special_token_184|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128193: AddedToken(\"<|reserved_special_token_185|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128194: AddedToken(\"<|reserved_special_token_186|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128195: AddedToken(\"<|reserved_special_token_187|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128196: AddedToken(\"<|reserved_special_token_188|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128197: AddedToken(\"<|reserved_special_token_189|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128198: AddedToken(\"<|reserved_special_token_190|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128199: AddedToken(\"<|reserved_special_token_191|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128200: AddedToken(\"<|reserved_special_token_192|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128201: AddedToken(\"<|reserved_special_token_193|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128202: AddedToken(\"<|reserved_special_token_194|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128203: AddedToken(\"<|reserved_special_token_195|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128204: AddedToken(\"<|reserved_special_token_196|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128205: AddedToken(\"<|reserved_special_token_197|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128206: AddedToken(\"<|reserved_special_token_198|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128207: AddedToken(\"<|reserved_special_token_199|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128208: AddedToken(\"<|reserved_special_token_200|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128209: AddedToken(\"<|reserved_special_token_201|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128210: AddedToken(\"<|reserved_special_token_202|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128211: AddedToken(\"<|reserved_special_token_203|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128212: AddedToken(\"<|reserved_special_token_204|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128213: AddedToken(\"<|reserved_special_token_205|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128214: AddedToken(\"<|reserved_special_token_206|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128215: AddedToken(\"<|reserved_special_token_207|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128216: AddedToken(\"<|reserved_special_token_208|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128217: AddedToken(\"<|reserved_special_token_209|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128218: AddedToken(\"<|reserved_special_token_210|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128219: AddedToken(\"<|reserved_special_token_211|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128220: AddedToken(\"<|reserved_special_token_212|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128221: AddedToken(\"<|reserved_special_token_213|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128222: AddedToken(\"<|reserved_special_token_214|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128223: AddedToken(\"<|reserved_special_token_215|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128224: AddedToken(\"<|reserved_special_token_216|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128225: AddedToken(\"<|reserved_special_token_217|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128226: AddedToken(\"<|reserved_special_token_218|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128227: AddedToken(\"<|reserved_special_token_219|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128228: AddedToken(\"<|reserved_special_token_220|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128229: AddedToken(\"<|reserved_special_token_221|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128230: AddedToken(\"<|reserved_special_token_222|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128231: AddedToken(\"<|reserved_special_token_223|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128232: AddedToken(\"<|reserved_special_token_224|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128233: AddedToken(\"<|reserved_special_token_225|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128234: AddedToken(\"<|reserved_special_token_226|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128235: AddedToken(\"<|reserved_special_token_227|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128236: AddedToken(\"<|reserved_special_token_228|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128237: AddedToken(\"<|reserved_special_token_229|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128238: AddedToken(\"<|reserved_special_token_230|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128239: AddedToken(\"<|reserved_special_token_231|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128240: AddedToken(\"<|reserved_special_token_232|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128241: AddedToken(\"<|reserved_special_token_233|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128242: AddedToken(\"<|reserved_special_token_234|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128243: AddedToken(\"<|reserved_special_token_235|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128244: AddedToken(\"<|reserved_special_token_236|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128245: AddedToken(\"<|reserved_special_token_237|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128246: AddedToken(\"<|reserved_special_token_238|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128247: AddedToken(\"<|reserved_special_token_239|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128248: AddedToken(\"<|reserved_special_token_240|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128249: AddedToken(\"<|reserved_special_token_241|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128250: AddedToken(\"<|reserved_special_token_242|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128251: AddedToken(\"<|reserved_special_token_243|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128252: AddedToken(\"<|reserved_special_token_244|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128253: AddedToken(\"<|reserved_special_token_245|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128254: AddedToken(\"<|reserved_special_token_246|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t128255: AddedToken(\"<|reserved_special_token_247|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            ")\n",
            "Tokenizer pad_token: <|finetune_right_pad_id|>, pad_token_id: 128004\n",
            "Model type: <class 'peft.peft_model.PeftModelForCausalLM'>\n",
            "Initializing DPOTrainer...\n",
            "DPOTrainer initialized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 11,573 | Num Epochs = 1 | Total steps = 100\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 24,313,856/3,000,000,000 (0.81% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 09:39, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>rewards / chosen</th>\n",
              "      <th>rewards / rejected</th>\n",
              "      <th>rewards / accuracies</th>\n",
              "      <th>rewards / margins</th>\n",
              "      <th>logps / chosen</th>\n",
              "      <th>logps / rejected</th>\n",
              "      <th>logits / chosen</th>\n",
              "      <th>logits / rejected</th>\n",
              "      <th>eval_logits / chosen</th>\n",
              "      <th>eval_logits / rejected</th>\n",
              "      <th>nll_loss</th>\n",
              "      <th>aux_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.693100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-153.422852</td>\n",
              "      <td>-208.524078</td>\n",
              "      <td>0.117732</td>\n",
              "      <td>0.178122</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.693100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-180.398056</td>\n",
              "      <td>-170.007004</td>\n",
              "      <td>-0.187644</td>\n",
              "      <td>0.135353</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.687100</td>\n",
              "      <td>0.012824</td>\n",
              "      <td>-0.000616</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.013440</td>\n",
              "      <td>-111.177040</td>\n",
              "      <td>-166.111328</td>\n",
              "      <td>0.011036</td>\n",
              "      <td>0.343426</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.678400</td>\n",
              "      <td>0.003322</td>\n",
              "      <td>-0.026822</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.030144</td>\n",
              "      <td>-150.158600</td>\n",
              "      <td>-186.937744</td>\n",
              "      <td>-0.037077</td>\n",
              "      <td>0.279276</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.674500</td>\n",
              "      <td>-0.012721</td>\n",
              "      <td>-0.051537</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.038816</td>\n",
              "      <td>-127.877853</td>\n",
              "      <td>-162.482300</td>\n",
              "      <td>-0.071527</td>\n",
              "      <td>0.097120</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.610000</td>\n",
              "      <td>-0.018829</td>\n",
              "      <td>-0.199598</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.180769</td>\n",
              "      <td>-237.268295</td>\n",
              "      <td>-211.266052</td>\n",
              "      <td>0.237227</td>\n",
              "      <td>0.499680</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.493500</td>\n",
              "      <td>-0.087766</td>\n",
              "      <td>-0.563288</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.475523</td>\n",
              "      <td>-403.237427</td>\n",
              "      <td>-269.731934</td>\n",
              "      <td>0.123187</td>\n",
              "      <td>0.037804</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.397100</td>\n",
              "      <td>-0.213510</td>\n",
              "      <td>-0.984486</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.770976</td>\n",
              "      <td>-251.754623</td>\n",
              "      <td>-241.050385</td>\n",
              "      <td>0.294447</td>\n",
              "      <td>-0.016132</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.254700</td>\n",
              "      <td>-0.111391</td>\n",
              "      <td>-1.528850</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.417459</td>\n",
              "      <td>-68.688828</td>\n",
              "      <td>-153.924164</td>\n",
              "      <td>-0.536586</td>\n",
              "      <td>0.132734</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.172900</td>\n",
              "      <td>-0.497857</td>\n",
              "      <td>-2.547520</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.049663</td>\n",
              "      <td>-343.761536</td>\n",
              "      <td>-300.357758</td>\n",
              "      <td>-0.377637</td>\n",
              "      <td>-0.414643</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.106700</td>\n",
              "      <td>-0.415825</td>\n",
              "      <td>-3.998318</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.582493</td>\n",
              "      <td>-127.201050</td>\n",
              "      <td>-228.806854</td>\n",
              "      <td>0.029323</td>\n",
              "      <td>-0.016505</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.076300</td>\n",
              "      <td>-0.380953</td>\n",
              "      <td>-4.643468</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.262516</td>\n",
              "      <td>-116.543663</td>\n",
              "      <td>-194.636612</td>\n",
              "      <td>-0.227015</td>\n",
              "      <td>0.123567</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.055400</td>\n",
              "      <td>-0.888838</td>\n",
              "      <td>-5.957354</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.068516</td>\n",
              "      <td>-116.406624</td>\n",
              "      <td>-190.527451</td>\n",
              "      <td>-0.559263</td>\n",
              "      <td>-0.300455</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.005400</td>\n",
              "      <td>-1.614739</td>\n",
              "      <td>-10.948063</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>9.333324</td>\n",
              "      <td>-140.129105</td>\n",
              "      <td>-304.067413</td>\n",
              "      <td>-0.324709</td>\n",
              "      <td>-0.131522</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.013500</td>\n",
              "      <td>-4.294772</td>\n",
              "      <td>-11.938310</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.643538</td>\n",
              "      <td>-322.633331</td>\n",
              "      <td>-335.549561</td>\n",
              "      <td>-0.352745</td>\n",
              "      <td>-0.354776</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>-1.908767</td>\n",
              "      <td>-18.207260</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>16.298492</td>\n",
              "      <td>-105.689224</td>\n",
              "      <td>-405.813354</td>\n",
              "      <td>-0.483677</td>\n",
              "      <td>-0.530484</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.308100</td>\n",
              "      <td>-6.779460</td>\n",
              "      <td>-17.635365</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>10.855906</td>\n",
              "      <td>-412.649628</td>\n",
              "      <td>-398.336639</td>\n",
              "      <td>-0.375755</td>\n",
              "      <td>-0.651622</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>-5.199090</td>\n",
              "      <td>-22.316641</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>17.117550</td>\n",
              "      <td>-217.829788</td>\n",
              "      <td>-435.663330</td>\n",
              "      <td>-0.402940</td>\n",
              "      <td>-0.398850</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>-6.645060</td>\n",
              "      <td>-20.340061</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>13.695001</td>\n",
              "      <td>-198.641357</td>\n",
              "      <td>-362.690735</td>\n",
              "      <td>-0.819661</td>\n",
              "      <td>-0.909848</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.005500</td>\n",
              "      <td>-6.714063</td>\n",
              "      <td>-25.988708</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>19.274643</td>\n",
              "      <td>-394.666321</td>\n",
              "      <td>-478.571045</td>\n",
              "      <td>-0.346351</td>\n",
              "      <td>-0.523414</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-7.196868</td>\n",
              "      <td>-34.897358</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>27.700491</td>\n",
              "      <td>-361.411865</td>\n",
              "      <td>-587.658691</td>\n",
              "      <td>-0.819019</td>\n",
              "      <td>-1.035882</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>-2.247837</td>\n",
              "      <td>-15.223104</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>12.975266</td>\n",
              "      <td>-73.298355</td>\n",
              "      <td>-223.892868</td>\n",
              "      <td>-1.213754</td>\n",
              "      <td>-1.191892</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.033600</td>\n",
              "      <td>-4.013400</td>\n",
              "      <td>-27.443663</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>23.430265</td>\n",
              "      <td>-256.750854</td>\n",
              "      <td>-488.535950</td>\n",
              "      <td>-1.160167</td>\n",
              "      <td>-1.078737</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.066300</td>\n",
              "      <td>-9.296932</td>\n",
              "      <td>-31.869617</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>22.572685</td>\n",
              "      <td>-386.440186</td>\n",
              "      <td>-526.489319</td>\n",
              "      <td>-0.769911</td>\n",
              "      <td>-1.035222</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.004200</td>\n",
              "      <td>-9.074646</td>\n",
              "      <td>-30.509121</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>21.434473</td>\n",
              "      <td>-191.757217</td>\n",
              "      <td>-445.273407</td>\n",
              "      <td>-1.234632</td>\n",
              "      <td>-1.141227</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-5.268369</td>\n",
              "      <td>-26.337547</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>21.069178</td>\n",
              "      <td>-123.225533</td>\n",
              "      <td>-406.559326</td>\n",
              "      <td>-0.902601</td>\n",
              "      <td>-1.185239</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.494900</td>\n",
              "      <td>-6.908614</td>\n",
              "      <td>-16.059370</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>9.150756</td>\n",
              "      <td>-211.449585</td>\n",
              "      <td>-259.965973</td>\n",
              "      <td>-0.890769</td>\n",
              "      <td>-0.853610</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.596836</td>\n",
              "      <td>-36.559147</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>27.962311</td>\n",
              "      <td>-248.483963</td>\n",
              "      <td>-634.969910</td>\n",
              "      <td>-0.605250</td>\n",
              "      <td>-0.728430</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-9.006859</td>\n",
              "      <td>-40.197746</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>31.190884</td>\n",
              "      <td>-226.890579</td>\n",
              "      <td>-631.228699</td>\n",
              "      <td>-0.510875</td>\n",
              "      <td>-1.032273</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.063900</td>\n",
              "      <td>-3.133555</td>\n",
              "      <td>-27.649574</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>24.516018</td>\n",
              "      <td>-162.575211</td>\n",
              "      <td>-420.693420</td>\n",
              "      <td>-1.296743</td>\n",
              "      <td>-1.270947</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.655180</td>\n",
              "      <td>-23.106171</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>19.450989</td>\n",
              "      <td>-126.138435</td>\n",
              "      <td>-379.203644</td>\n",
              "      <td>-0.508711</td>\n",
              "      <td>-0.816572</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.259700</td>\n",
              "      <td>-11.273950</td>\n",
              "      <td>-31.138262</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>19.864313</td>\n",
              "      <td>-777.095337</td>\n",
              "      <td>-588.086731</td>\n",
              "      <td>-0.654127</td>\n",
              "      <td>-0.609943</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.006000</td>\n",
              "      <td>-2.444886</td>\n",
              "      <td>-20.813814</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>18.368929</td>\n",
              "      <td>-94.160606</td>\n",
              "      <td>-329.404419</td>\n",
              "      <td>-0.925960</td>\n",
              "      <td>-1.088259</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.005900</td>\n",
              "      <td>-10.041009</td>\n",
              "      <td>-28.845768</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>18.804758</td>\n",
              "      <td>-429.750305</td>\n",
              "      <td>-482.658997</td>\n",
              "      <td>-0.118922</td>\n",
              "      <td>-0.455590</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>-8.025740</td>\n",
              "      <td>-29.132599</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>21.106861</td>\n",
              "      <td>-277.776062</td>\n",
              "      <td>-512.256226</td>\n",
              "      <td>-0.596968</td>\n",
              "      <td>-0.629086</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.086600</td>\n",
              "      <td>-2.448010</td>\n",
              "      <td>-21.177689</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>18.729679</td>\n",
              "      <td>-129.411148</td>\n",
              "      <td>-387.254822</td>\n",
              "      <td>-0.842084</td>\n",
              "      <td>-0.589988</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.687200</td>\n",
              "      <td>-3.937753</td>\n",
              "      <td>-22.211952</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>18.274199</td>\n",
              "      <td>-194.123840</td>\n",
              "      <td>-372.308990</td>\n",
              "      <td>-0.014332</td>\n",
              "      <td>-0.319041</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.026300</td>\n",
              "      <td>-3.906588</td>\n",
              "      <td>-25.568089</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>21.661503</td>\n",
              "      <td>-173.055145</td>\n",
              "      <td>-432.186157</td>\n",
              "      <td>0.134372</td>\n",
              "      <td>-0.115707</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.017200</td>\n",
              "      <td>-0.995629</td>\n",
              "      <td>-18.606428</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>17.610800</td>\n",
              "      <td>-83.105331</td>\n",
              "      <td>-333.728638</td>\n",
              "      <td>-0.027263</td>\n",
              "      <td>-0.240153</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.047800</td>\n",
              "      <td>-4.625096</td>\n",
              "      <td>-17.391329</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>12.766232</td>\n",
              "      <td>-393.854858</td>\n",
              "      <td>-328.892944</td>\n",
              "      <td>0.121052</td>\n",
              "      <td>-0.519643</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.003100</td>\n",
              "      <td>-2.513038</td>\n",
              "      <td>-18.220219</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>15.707181</td>\n",
              "      <td>-117.233353</td>\n",
              "      <td>-332.234924</td>\n",
              "      <td>-0.180924</td>\n",
              "      <td>-0.065946</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.655571</td>\n",
              "      <td>-23.899828</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>21.244259</td>\n",
              "      <td>-337.043701</td>\n",
              "      <td>-451.116852</td>\n",
              "      <td>0.081838</td>\n",
              "      <td>-0.092498</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.002200</td>\n",
              "      <td>-2.999066</td>\n",
              "      <td>-23.208271</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>20.209209</td>\n",
              "      <td>-158.894409</td>\n",
              "      <td>-398.829163</td>\n",
              "      <td>-0.384037</td>\n",
              "      <td>-0.309797</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.911241</td>\n",
              "      <td>-27.631016</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>23.719772</td>\n",
              "      <td>-273.869690</td>\n",
              "      <td>-540.724243</td>\n",
              "      <td>0.383874</td>\n",
              "      <td>0.009772</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.712239</td>\n",
              "      <td>-27.644196</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>24.931957</td>\n",
              "      <td>-217.269211</td>\n",
              "      <td>-500.135406</td>\n",
              "      <td>-0.016335</td>\n",
              "      <td>-0.262482</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>-4.115623</td>\n",
              "      <td>-17.495399</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>13.379776</td>\n",
              "      <td>-341.542175</td>\n",
              "      <td>-332.461975</td>\n",
              "      <td>-0.277634</td>\n",
              "      <td>-0.270749</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>-4.142870</td>\n",
              "      <td>-21.489929</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>17.347059</td>\n",
              "      <td>-240.931976</td>\n",
              "      <td>-422.181641</td>\n",
              "      <td>0.231200</td>\n",
              "      <td>0.082304</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>-2.681664</td>\n",
              "      <td>-25.139668</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>22.458004</td>\n",
              "      <td>-155.352936</td>\n",
              "      <td>-421.566650</td>\n",
              "      <td>0.172272</td>\n",
              "      <td>-0.033861</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>-3.271920</td>\n",
              "      <td>-23.144093</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>19.872169</td>\n",
              "      <td>-220.954453</td>\n",
              "      <td>-493.570526</td>\n",
              "      <td>0.006892</td>\n",
              "      <td>-0.044731</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>-5.154077</td>\n",
              "      <td>-23.231770</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>18.077694</td>\n",
              "      <td>-290.501709</td>\n",
              "      <td>-443.152832</td>\n",
              "      <td>0.062079</td>\n",
              "      <td>0.046770</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>-2.651371</td>\n",
              "      <td>-23.148067</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>20.496696</td>\n",
              "      <td>-217.698013</td>\n",
              "      <td>-406.757751</td>\n",
              "      <td>0.315451</td>\n",
              "      <td>-0.149973</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>-2.026654</td>\n",
              "      <td>-18.473101</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>16.446445</td>\n",
              "      <td>-212.465698</td>\n",
              "      <td>-376.952148</td>\n",
              "      <td>0.014482</td>\n",
              "      <td>-0.231746</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>-2.525106</td>\n",
              "      <td>-19.222427</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>16.697323</td>\n",
              "      <td>-257.818237</td>\n",
              "      <td>-375.822113</td>\n",
              "      <td>0.273521</td>\n",
              "      <td>-0.116242</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-2.995564</td>\n",
              "      <td>-16.277712</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>13.282148</td>\n",
              "      <td>-230.865402</td>\n",
              "      <td>-342.251953</td>\n",
              "      <td>-0.332797</td>\n",
              "      <td>-0.399189</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.809461</td>\n",
              "      <td>-23.656254</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>19.846790</td>\n",
              "      <td>-187.176239</td>\n",
              "      <td>-390.064880</td>\n",
              "      <td>0.401610</td>\n",
              "      <td>-0.163188</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.006900</td>\n",
              "      <td>-1.432031</td>\n",
              "      <td>-17.863686</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>16.431654</td>\n",
              "      <td>-215.456924</td>\n",
              "      <td>-351.063446</td>\n",
              "      <td>-0.328784</td>\n",
              "      <td>-0.039221</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>-2.452480</td>\n",
              "      <td>-21.050732</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>18.598251</td>\n",
              "      <td>-180.900925</td>\n",
              "      <td>-353.358093</td>\n",
              "      <td>0.065793</td>\n",
              "      <td>0.114897</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>-2.663375</td>\n",
              "      <td>-18.696499</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>16.033123</td>\n",
              "      <td>-370.080383</td>\n",
              "      <td>-396.459595</td>\n",
              "      <td>-0.181189</td>\n",
              "      <td>-0.232037</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.001400</td>\n",
              "      <td>-2.848435</td>\n",
              "      <td>-23.679115</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>20.830679</td>\n",
              "      <td>-373.785583</td>\n",
              "      <td>-456.869690</td>\n",
              "      <td>0.385989</td>\n",
              "      <td>-0.130544</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.676014</td>\n",
              "      <td>-28.140835</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>25.464819</td>\n",
              "      <td>-163.785797</td>\n",
              "      <td>-501.689789</td>\n",
              "      <td>0.046215</td>\n",
              "      <td>0.011305</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.005900</td>\n",
              "      <td>-4.238875</td>\n",
              "      <td>-26.632761</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>22.393887</td>\n",
              "      <td>-323.869446</td>\n",
              "      <td>-480.386047</td>\n",
              "      <td>-0.172594</td>\n",
              "      <td>-0.131485</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.165244</td>\n",
              "      <td>-19.564882</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>17.399639</td>\n",
              "      <td>-122.805481</td>\n",
              "      <td>-338.469360</td>\n",
              "      <td>-0.861211</td>\n",
              "      <td>-0.661944</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-4.181275</td>\n",
              "      <td>-33.549232</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>29.367958</td>\n",
              "      <td>-313.303040</td>\n",
              "      <td>-572.759705</td>\n",
              "      <td>0.010373</td>\n",
              "      <td>0.034288</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.204748</td>\n",
              "      <td>-33.005585</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>29.800837</td>\n",
              "      <td>-266.199097</td>\n",
              "      <td>-523.595703</td>\n",
              "      <td>0.073458</td>\n",
              "      <td>-0.195494</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>-3.377150</td>\n",
              "      <td>-26.476624</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>23.099472</td>\n",
              "      <td>-180.620209</td>\n",
              "      <td>-459.592651</td>\n",
              "      <td>-0.203232</td>\n",
              "      <td>-0.033724</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-5.813009</td>\n",
              "      <td>-34.218464</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>28.405453</td>\n",
              "      <td>-389.871887</td>\n",
              "      <td>-598.286743</td>\n",
              "      <td>0.028154</td>\n",
              "      <td>-0.064489</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-2.604763</td>\n",
              "      <td>-27.519291</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>24.914530</td>\n",
              "      <td>-145.845657</td>\n",
              "      <td>-471.388885</td>\n",
              "      <td>0.124551</td>\n",
              "      <td>-0.201512</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.011300</td>\n",
              "      <td>-3.549637</td>\n",
              "      <td>-30.399200</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>26.849564</td>\n",
              "      <td>-173.542709</td>\n",
              "      <td>-489.287354</td>\n",
              "      <td>-0.438338</td>\n",
              "      <td>-0.425105</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-6.515379</td>\n",
              "      <td>-28.751492</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>22.236111</td>\n",
              "      <td>-509.817902</td>\n",
              "      <td>-545.277832</td>\n",
              "      <td>-0.005637</td>\n",
              "      <td>-0.096300</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.421530</td>\n",
              "      <td>-21.429161</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>18.007631</td>\n",
              "      <td>-450.464905</td>\n",
              "      <td>-423.704346</td>\n",
              "      <td>-0.107714</td>\n",
              "      <td>-0.086816</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-5.192076</td>\n",
              "      <td>-31.779802</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>26.587727</td>\n",
              "      <td>-286.681671</td>\n",
              "      <td>-615.690857</td>\n",
              "      <td>0.014161</td>\n",
              "      <td>-0.176281</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-3.122272</td>\n",
              "      <td>-30.303064</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>27.180794</td>\n",
              "      <td>-154.951477</td>\n",
              "      <td>-476.187683</td>\n",
              "      <td>0.119093</td>\n",
              "      <td>0.054375</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-4.860532</td>\n",
              "      <td>-24.257687</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>19.397152</td>\n",
              "      <td>-289.986145</td>\n",
              "      <td>-440.873169</td>\n",
              "      <td>0.176839</td>\n",
              "      <td>-0.140992</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>-2.637383</td>\n",
              "      <td>-22.209286</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>19.571903</td>\n",
              "      <td>-101.105118</td>\n",
              "      <td>-372.080750</td>\n",
              "      <td>-0.600286</td>\n",
              "      <td>-0.236984</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-4.083792</td>\n",
              "      <td>-33.206459</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>29.122665</td>\n",
              "      <td>-149.669067</td>\n",
              "      <td>-552.434509</td>\n",
              "      <td>-0.418252</td>\n",
              "      <td>-0.033531</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-4.012054</td>\n",
              "      <td>-26.000265</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>21.988213</td>\n",
              "      <td>-248.738892</td>\n",
              "      <td>-424.527893</td>\n",
              "      <td>0.292812</td>\n",
              "      <td>-0.146713</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-2.873889</td>\n",
              "      <td>-26.754896</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>23.881008</td>\n",
              "      <td>-293.974976</td>\n",
              "      <td>-512.261292</td>\n",
              "      <td>0.046240</td>\n",
              "      <td>-0.086292</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.933816</td>\n",
              "      <td>-25.228237</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>23.294420</td>\n",
              "      <td>-126.397537</td>\n",
              "      <td>-409.727142</td>\n",
              "      <td>0.287535</td>\n",
              "      <td>-0.257182</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.665727</td>\n",
              "      <td>-27.997814</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>26.332088</td>\n",
              "      <td>-218.886673</td>\n",
              "      <td>-508.645386</td>\n",
              "      <td>0.324981</td>\n",
              "      <td>0.014750</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.012300</td>\n",
              "      <td>-8.997473</td>\n",
              "      <td>-26.538879</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>17.541409</td>\n",
              "      <td>-270.463226</td>\n",
              "      <td>-442.160065</td>\n",
              "      <td>0.010774</td>\n",
              "      <td>-0.473754</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.686928</td>\n",
              "      <td>-31.155682</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>28.468754</td>\n",
              "      <td>-395.557648</td>\n",
              "      <td>-552.251038</td>\n",
              "      <td>-0.115415</td>\n",
              "      <td>-0.376708</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-5.147190</td>\n",
              "      <td>-28.590691</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>23.443501</td>\n",
              "      <td>-330.734222</td>\n",
              "      <td>-493.412964</td>\n",
              "      <td>-0.147428</td>\n",
              "      <td>-0.024776</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>-0.779625</td>\n",
              "      <td>-20.353706</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>19.574081</td>\n",
              "      <td>-192.422211</td>\n",
              "      <td>-365.162933</td>\n",
              "      <td>0.136622</td>\n",
              "      <td>-0.085796</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.003800</td>\n",
              "      <td>-3.402561</td>\n",
              "      <td>-22.841162</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>19.438601</td>\n",
              "      <td>-145.633942</td>\n",
              "      <td>-423.853271</td>\n",
              "      <td>-0.265169</td>\n",
              "      <td>-0.259905</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>-2.307723</td>\n",
              "      <td>-15.932949</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>13.625225</td>\n",
              "      <td>-181.085297</td>\n",
              "      <td>-308.461212</td>\n",
              "      <td>-0.099768</td>\n",
              "      <td>-0.333879</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.993976</td>\n",
              "      <td>-24.276930</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>22.282953</td>\n",
              "      <td>-158.573090</td>\n",
              "      <td>-446.603973</td>\n",
              "      <td>0.379725</td>\n",
              "      <td>-0.039148</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.558929</td>\n",
              "      <td>-24.932102</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>22.373173</td>\n",
              "      <td>-294.399689</td>\n",
              "      <td>-451.598755</td>\n",
              "      <td>-0.390669</td>\n",
              "      <td>-0.430121</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.775439</td>\n",
              "      <td>-23.401066</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>20.625626</td>\n",
              "      <td>-214.298218</td>\n",
              "      <td>-415.739075</td>\n",
              "      <td>0.246590</td>\n",
              "      <td>-0.166728</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-5.814333</td>\n",
              "      <td>-27.075518</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>21.261185</td>\n",
              "      <td>-252.507355</td>\n",
              "      <td>-439.680359</td>\n",
              "      <td>-0.738751</td>\n",
              "      <td>-0.503680</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-3.059333</td>\n",
              "      <td>-22.101574</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>19.042240</td>\n",
              "      <td>-242.974182</td>\n",
              "      <td>-390.266846</td>\n",
              "      <td>0.163410</td>\n",
              "      <td>-0.375596</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.033386</td>\n",
              "      <td>-30.861628</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>27.828239</td>\n",
              "      <td>-229.331787</td>\n",
              "      <td>-498.494659</td>\n",
              "      <td>-0.032618</td>\n",
              "      <td>-0.138759</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-4.807876</td>\n",
              "      <td>-31.456100</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>26.648226</td>\n",
              "      <td>-317.795074</td>\n",
              "      <td>-554.707764</td>\n",
              "      <td>-0.034252</td>\n",
              "      <td>-0.219603</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.086600</td>\n",
              "      <td>-4.792253</td>\n",
              "      <td>-26.731731</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>21.939480</td>\n",
              "      <td>-188.234695</td>\n",
              "      <td>-437.748047</td>\n",
              "      <td>-0.201890</td>\n",
              "      <td>-0.655650</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.687421</td>\n",
              "      <td>-25.537472</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>23.850052</td>\n",
              "      <td>-169.958801</td>\n",
              "      <td>-422.234589</td>\n",
              "      <td>-0.203044</td>\n",
              "      <td>-0.242450</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.001400</td>\n",
              "      <td>-1.335764</td>\n",
              "      <td>-18.072039</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>16.736273</td>\n",
              "      <td>-167.763702</td>\n",
              "      <td>-310.927673</td>\n",
              "      <td>0.487961</td>\n",
              "      <td>0.027596</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.379801</td>\n",
              "      <td>-22.388716</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>21.008915</td>\n",
              "      <td>-232.768112</td>\n",
              "      <td>-460.013550</td>\n",
              "      <td>0.472494</td>\n",
              "      <td>0.154526</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>-2.916618</td>\n",
              "      <td>-21.100523</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>18.183907</td>\n",
              "      <td>-169.790771</td>\n",
              "      <td>-323.203369</td>\n",
              "      <td>-0.443481</td>\n",
              "      <td>-0.540865</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.916840</td>\n",
              "      <td>-24.659832</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>21.742992</td>\n",
              "      <td>-123.059433</td>\n",
              "      <td>-415.712799</td>\n",
              "      <td>0.212308</td>\n",
              "      <td>-0.009887</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.495000</td>\n",
              "      <td>-4.432765</td>\n",
              "      <td>-28.028126</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>23.595360</td>\n",
              "      <td>-190.511688</td>\n",
              "      <td>-424.472900</td>\n",
              "      <td>-0.302001</td>\n",
              "      <td>-0.591145</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.003400</td>\n",
              "      <td>-1.145759</td>\n",
              "      <td>-17.395931</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>16.250174</td>\n",
              "      <td>-53.415474</td>\n",
              "      <td>-262.706146</td>\n",
              "      <td>-0.323026</td>\n",
              "      <td>-0.614973</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DPO training finished.\n",
            "TrainOutput(global_step=100, training_loss=0.08371178271720207, metrics={'train_runtime': 587.0577, 'train_samples_per_second': 1.363, 'train_steps_per_second': 0.17, 'total_flos': 0.0, 'train_loss': 0.08371178271720207, 'epoch': 0.06912044237083118})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Report final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{dpo_trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(dpo_trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMEARciW-ROP",
        "outputId": "ff2c4367-7bb6-4a87-ee16-073e5d397977"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "587.0577 seconds used for training.\n",
            "9.78 minutes used for training.\n",
            "Peak reserved memory = 16.885 GB.\n",
            "Peak reserved memory for training = 5.604 GB.\n",
            "Peak reserved memory % of max memory = 76.192 %.\n",
            "Peak reserved memory for training % of max memory = 25.288 %.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 11: Evaluate the - fine-tuned with DPO - model\n",
        "\n",
        "Use the same evaluation dataset with the previous model and compare the three models (base, SFT, DPO) performance using various metrics."
      ],
      "metadata": {
        "id": "16AK9cgTMXdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 12: Design Evaluation Prompts to perform Qualitative analysis\n",
        "\n",
        "### Exercise Instructions:\n",
        "1. **Design Evaluation Prompts**:\n",
        "   - Create at least 7 distinct prompts that test various abilities, such as:\n",
        "     - Instruction-following (e.g., \"Explain the difference between LoRA and fine-tuning.\")\n",
        "     - Creativity (e.g., \"Write a short story about a time-traveling musician.\")\n",
        "     - Factual accuracy (e.g., \"What is the capital of Japan?\")\n",
        "     - Problem-solving (e.g., \"Solve the equation: x + 5 = 10.\")\n",
        "   - Ensure a mix of simple and complex prompts.\n",
        "\n",
        "2. **Generate Responses**:\n",
        "   - Use the `generate_response()` function to get outputs from `base_model`, `sft_model`, and `dpo_model` for each prompt.\n",
        "\n",
        "3. **Perform Qualitative Analysis**:\n",
        "   - Compare the responses qualitatively across the models based on:\n",
        "     - **Coherence**: Is the response logical and well-structured?\n",
        "     - **Relevance**: Does it directly address the prompt?\n",
        "     - **Fluency**: Is the response grammatically correct and readable?\n",
        "     - **Accuracy**: For factual questions, is the information correct?\n",
        "     - **Creativity**: For open-ended prompts, is the response engaging and original?"
      ],
      "metadata": {
        "id": "sFPFCRhbMXdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 1: \"Explain the difference between LoRA and fine-tuning.\"\n",
        "# Question 2: \"Write a short story about a time-traveling musician.\"\n",
        "# Question 3: \"What is the capital of Japan?\"\n",
        "# Question 4: \"Solve the equation: x + 5 = 10.\"\n",
        "# Question 5: \"Translate the following English sentence into Spanish: “The school is painted red.”\"\n",
        "# Question 6: \"If an animal barks, is it a dog or a cat? Explain your reasoning.\"\n",
        "# Question 7: \"Is the National Technical University of Athens well-established?\"\n",
        "\n",
        "questions = [\n",
        "    \"Explain the difference between LoRA and fine-tuning.\",\n",
        "    \"Write a short story about a time-traveling musician.\",\n",
        "    \"What is the capital of Japan?\",\n",
        "    \"Solve the equation: x + 5 = 10.\",\n",
        "    \"Translate the following English sentence into Spanish: “The school is painted red.”\",\n",
        "    \"If an animal barks, is it a dog or a cat? Explain your reasoning.\",\n",
        "    \"Is the National Technical University of Athens well-established?\"\n",
        "]\n",
        "\n",
        "models = {\n",
        "    \"base\": (base_model, base_tokenizer),\n",
        "    \"sft\": (sft_model, sft_tokenizer)\n",
        "}\n",
        "\n",
        "# Loop through each question and each model\n",
        "for i, question in enumerate(questions, 1):\n",
        "    print(f\"\\n=== Question {i}: {question} ===\")\n",
        "    for model_name, (model, tokenizer) in models.items():\n",
        "        response = generate_response(question, model, tokenizer, max_new_tokens=512)\n",
        "        print(f\"\\nModel: {model_name}\")\n",
        "        print(f\"Response:\\n{response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiGtkUoMhXBT",
        "outputId": "bb00b243-f6e7-4c27-dd79-461c6d65e39f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Question 1: Explain the difference between LoRA and fine-tuning. ===\n",
            "\n",
            "Model: base\n",
            "Response:\n",
            "Explain the difference between LoRA and fine-tuning. two AI models\n",
            "LoRA and fine-tuning are two different approaches to improve the performance of AI models. While both methods aim to adapt models to specific tasks or datasets, they differ in their underlying architecture and the nature of the adaptations.\n",
            "\n",
            "**LoRA (Low-Rank Adaptation)**\n",
            "\n",
            "LoRA is a type of low-rank adaptation that modifies the weights of a pre-trained model to better fit a new task or dataset. The idea is to learn a low-rank matrix that represents the adaptation process, which is then applied to the original weights. This process can be viewed as a linear transformation of the weights.\n",
            "\n",
            "LoRA works by:\n",
            "\n",
            "1. Pre-training a model on a large dataset\n",
            "2. Selecting a subset of the weights to be adapted\n",
            "3. Learning a low-rank matrix (typically 2D) that represents the adaptation process\n",
            "4. Applying the adaptation matrix to the selected weights\n",
            "\n",
            "The resulting adaptation is a new set of weights that are optimized for the new task or dataset.\n",
            "\n",
            "**Fine-Tuning**\n",
            "\n",
            "Fine-tuning is a more comprehensive approach that involves updating all the weights of a pre-trained model to better fit a new task or dataset. The idea is to learn a new set of weights that are optimized for the specific task or dataset.\n",
            "\n",
            "Fine-tuning works by:\n",
            "\n",
            "1. Pre-training a model on a large dataset\n",
            "2. Updating all the weights of the model using a smaller learning rate\n",
            "3. Training the model on the new task or dataset\n",
            "\n",
            "The resulting fine-tuned model is a new set of weights that are optimized for the specific task or dataset.\n",
            "\n",
            "**Key differences**\n",
            "\n",
            "* LoRA adapts a subset of the weights, while fine-tuning updates all the weights.\n",
            "* LoRA uses a low-rank matrix to represent the adaptation process, while fine-tuning uses a more traditional optimization algorithm.\n",
            "* LoRA is typically more efficient than fine-tuning, especially for large models, as it requires less computational resources.\n",
            "\n",
            "**When to use each**\n",
            "\n",
            "* Use LoRA when:\n",
            "\t+ You have a pre-trained model that needs to be adapted to a specific task or dataset.\n",
            "\t+ You want to adapt a subset of the weights to reduce the computational resources required.\n",
            "\t+ You want to preserve the original weights and only update a subset of them.\n",
            "* Use fine-tuning when:\n",
            "\t+ You want to adapt all the weights of a pre-trained model to a specific task or dataset.\n",
            "\t+ You want to optimize the entire model for the new task or dataset.\n",
            "\t+ You don\n",
            "\n",
            "Model: sft\n",
            "Response:\n",
            "Explain the difference between LoRA and fine-tuning. Both are machine learning approaches used for deep learning models.\n",
            "\n",
            "Lora (Low-Rank Approximation) is a technique that reduces the dimensionality of a deep neural network by reducing the number of parameters in the neural network. This is achieved by approximating the original neural network with a lower-dimensional version of it. This lower-dimensional version is known as the LoRA model. The LoRA model is a simplified version of the original neural network, with reduced parameters.\n",
            "\n",
            "Fine-tuning is a technique used to adapt a pre-trained model to a new task. It involves adding a new task-specific layer on top of the pre-trained model and training the entire model on the new task. This is done to improve the performance of the pre-trained model on the new task. The idea is that the pre-trained model already has learned some general knowledge, and adding a new task-specific layer allows it to generalize to the new task.\n",
            "\n",
            "In summary, LoRA is a technique that reduces the dimensionality of a deep neural network, while fine-tuning is a technique that adapts a pre-trained model to a new task. Both are machine learning approaches used for deep learning models, but they serve different purposes and have different implications for the performance and efficiency of the model. \n",
            "\n",
            "Answer:\n",
            "LoRA and fine-tuning are both machine learning approaches used for deep learning models. However, they serve different purposes and have different implications for the performance and efficiency of the model. LoRA reduces the dimensionality of a deep neural network by approximating the original neural network with a lower-dimensional version of it, while fine-tuning adapts a pre-trained model to a new task by adding a new task-specific layer on top of the pre-trained model and training the entire model on the new task. Therefore, the choice between LoRA and fine-tuning depends on the specific requirements and goals of the project. \n",
            "\n",
            "It is also worth noting that both LoRA and fine-tuning can improve the performance and efficiency of a deep neural network model. LoRA can reduce the computational complexity of the model, while fine-tuning can improve the performance of the pre-trained model on the new task. However, the choice between the two approaches depends on the specific requirements and goals of the project. \n",
            "\n",
            "In summary, LoRA and fine-tuning are both machine learning approaches used for deep learning models, but they serve different purposes and have different implications for the performance and efficiency of the model. The choice between the two approaches depends on the specific requirements and goals of the project. \n",
            "\n",
            "Answer:\n",
            "LoRA (\n",
            "\n",
            "=== Question 2: Write a short story about a time-traveling musician. ===\n",
            "\n",
            "Model: base\n",
            "Response:\n",
            "Write a short story about a time-traveling musician. Max is a free-spirited musician with a passion for the 1960s. He's always been fascinated by the music and culture of that era. One day, he discovers a way to travel through time, and he's immediately drawn to the Summer of Love in San Francisco.\n",
            "\n",
            "As he steps into the Haight-Ashbury neighborhood, Max is struck by the vibrant colors and sounds of the era. He's in awe of the peace signs, the tie-dye shirts, and the long-haired hippies. He feels like he's finally found his tribe.\n",
            "\n",
            "Max is approached by a young woman named Lily, who's playing her guitar on the street corner. She invites him to join her, and they start playing together. The music is like nothing Max has ever heard before – it's a fusion of folk, rock, and psychedelic sounds that speak directly to his soul.\n",
            "\n",
            "As the day goes on, Max becomes more and more immersed in the scene. He attends a gathering at the Diggers' Free Store, where he meets Allen Ginsberg, Timothy Leary, and other famous figures of the era. He's amazed by their passion for social change and their commitment to peace and love.\n",
            "\n",
            "As the sun sets over the Bay, Max realizes that he's found a new home – not just in San Francisco, but in the spirit of the Summer of Love. He decides to stay and become a part of the community, playing music and spreading the message of love and peace.\n",
            "\n",
            "Years later, Max returns to his own time, but the memories of that magical summer stay with him forever. He carries the lessons of the Summer of Love with him, and he continues to spread the message of love and peace through his music.\n",
            "\n",
            "Max's journey through time had been a once-in-a-lifetime experience, but the impact of that summer stay would be felt for the rest of his life. He had found his tribe, and he knew that he would always be a part of the Summer of Love.\n",
            "\n",
            "Model: sft\n",
            "Response:\n",
            "Write a short story about a time-traveling musician. who is also a descendant of the ancient Egyptians.  Here is the different parts of the story:\n",
            "\n",
            "**Part 1: The Ancestral Connection**\n",
            "In the heart of modern-day Cairo, there lived a young musician named Amir. Amir was a direct descendant of the ancient Egyptian pharaohs and possessed a deep understanding of the mystical energies that coursed through the Nile. He was also a talented musician, able to weave the sounds of the desert into enchanting melodies. Amir's music was said to have the power to awaken the gods, and many believed that he was the chosen one, destined to fulfill an ancient prophecy.\n",
            "\n",
            "**Part 2: The Time-Traveling Adventure**\n",
            "One day, Amir stumbled upon an ancient artifact hidden deep within the labyrinthine catacombs of Cairo. The artifact, a golden ankh, radiated a strange energy that seemed to connect Amir to a realm beyond the confines of time and space. As he held the ankh, Amir felt an extraordinary power coursing through his veins, and he knew that he was meant to embark on a journey through the ages.\n",
            "\n",
            "**Part 3: The Ancient Egyptian Odyssey**\n",
            "With the golden ankh in hand, Amir found himself transported to the banks of the Nile, where he stood among the pyramids of Giza. The air was filled with the sweet scent of lotus flowers, and the gods themselves seemed to be watching him. Amir began to play his lyre, and the sounds that flowed from it awakened the ancient gods, who descended upon him with gifts of wisdom and power. The pharaohs themselves welcomed Amir as a long-lost descendant, and together they embarked on a journey through the realms of time, exploring the mysteries of the universe.\n",
            "\n",
            "**Part 4: The Prophecy Fulfilled**\n",
            "As Amir journeyed through the ages, he encountered the great pharaohs of old, each one revealing to him the secrets of the universe. He played his lyre for the gods, and they granted him their wisdom and power. He walked among the stars, and he saw the birth of the cosmos. And when he finally returned to the present, Amir knew that his journey had fulfilled the prophecy. The ancient Egyptians had chosen him to be their champion, and he had carried the torch of their wisdom into the modern world.\n",
            "\n",
            "**Part 5: The Legacy of the Golden Ankh**\n",
            "The golden ankh remained with Amir, a symbol of his connection to the ancient Egyptians and the time-traveling adventure that had changed his life. He used\n",
            "\n",
            "=== Question 3: What is the capital of Japan? ===\n",
            "\n",
            "Model: base\n",
            "Response:\n",
            "What is the capital of Japan? Tokyo\n",
            "What is the capital of France? Paris\n",
            "What is the capital of Germany? Berlin\n",
            "What is the capital of Italy? Rome\n",
            "What is the capital of China? Beijing\n",
            "What is the capital of the United States? Washington D.C.\n",
            "What is the capital of Australia? Canberra\n",
            "What is the capital of Brazil? Brasília\n",
            "What is the capital of Russia? Moscow\n",
            "What is the capital of South Africa? Pretoria\n",
            "What is the capital of Egypt? Cairo\n",
            "What is the capital of India? New Delhi\n",
            "What is the capital of Mexico? Mexico City\n",
            "What is the capital of Poland? Warsaw\n",
            "What is the capital of South Korea? Seoul\n",
            "What is the capital of Turkey? Ankara\n",
            "What is the capital of Vietnam? Hanoi\n",
            "What is the capital of Saudi Arabia? Riyadh\n",
            "What is the capital of Thailand? Bangkok\n",
            "What is the capital of United Arab Emirates? Abu Dhabi\n",
            "What is the capital of Spain? Madrid\n",
            "What is the capital of Switzerland? Bern\n",
            "What is the capital of Sweden? Stockholm\n",
            "What is the capital of Norway? Oslo\n",
            "What is the capital of Denmark? Copenhagen\n",
            "What is the capital of Finland? Helsinki\n",
            "What is the capital of Greece? Athens\n",
            "What is the capital of Portugal? Lisbon\n",
            "What is the capital of Cyprus? Nicosia\n",
            "What is the capital of Malta? Valletta\n",
            "What is the capital of Slovenia? Ljubljana\n",
            "What is the capital of Croatia? Zagreb\n",
            "What is the capital of Bosnia and Herzegovina? Sarajevo\n",
            "What is the capital of Kosovo? Pristina\n",
            "What is the capital of North Macedonia? Skopje\n",
            "What is the capital of Albania? Tirana\n",
            "What is the capital of Armenia? Yerevan\n",
            "What is the capital of Azerbaijan? Baku\n",
            "What is the capital of Belarus? Minsk\n",
            "What is the capital of Georgia? Tbilisi\n",
            "What is the capital of Kazakhstan? Astana\n",
            "What is the capital of Kyrgyzstan? Bishkek\n",
            "What is the capital of Moldova? Chișinău\n",
            "What is the capital of Mongolia: Ulaanbaatar\n",
            "What is the capital of Nepal: Kathmandu\n",
            "What is the capital of Pakistan: Islamabad\n",
            "What is the capital of Philippines: Manila\n",
            "What is the capital of Singapore: Singapore\n",
            "What is the capital of Sri Lanka: Colombo\n",
            "What is the capital of Tajikistan: Dushanbe\n",
            "What is the capital\n",
            "\n",
            "Model: sft\n",
            "Response:\n",
            "What is the capital of Japan? Tokyo\n",
            "Tokyo is the capital city of Japan. It is a metropolis located on the eastern coast of the Japanese island of Honshu. Tokyo is the largest city in Japan and the country's main economic, financial, and cultural center. It is also the capital city of Japan's 23 special wards, which are the main administrative divisions of the country.\n",
            "\n",
            "The capital of Japan is also known as Edo, which is the old name for Tokyo. The name \"Tokyo\" is a combination of the words \"Tō\" and \"Kyō,\" which means \"Eastern Capital\" in Japanese.\n",
            "\n",
            "Tokyo is a bustling metropolis with a population of over 38 million people. It is known for its vibrant culture, rich history, and modern infrastructure. The city is home to many famous landmarks, including the Tokyo Tower, the Imperial Palace, and the Meiji Shrine.\n",
            "\n",
            "In addition to its cultural and historical significance, Tokyo is also a major economic and financial center. The city is home to many large corporations, including Sony, Toshiba, and Honda. It is also a major hub for international trade and commerce.\n",
            "\n",
            "Overall, Tokyo is a fascinating city that offers a unique blend of traditional and modern culture. It is a must-visit destination for anyone interested in exploring Japan's rich history and culture.\n",
            "\n",
            "=== Question 4: Solve the equation: x + 5 = 10. ===\n",
            "\n",
            "Model: base\n",
            "Response:\n",
            "Solve the equation: x + 5 = 10. (x is a real number)\n",
            "x + 5 = 10\n",
            "Subtract 5 from both sides of the equation.\n",
            "x + 5 - 5 = 10 - 5\n",
            "x = 5\n",
            "The solution to the equation is x = 5.\n",
            "\n",
            "Model: sft\n",
            "Response:\n",
            "Solve the equation: x + 5 = 10. \n",
            "\n",
            "## Step 1: Subtract 5 from both sides of the equation\n",
            "Subtracting 5 from both sides of the equation x + 5 = 10 will give us the value of x.\n",
            "\n",
            "## Step 2: Calculate the value of x\n",
            "x + 5 - 5 = 10 - 5\n",
            "x = 5\n",
            "\n",
            "The final answer is: $\\boxed{5}$\n",
            "\n",
            "=== Question 5: Translate the following English sentence into Spanish: “The school is painted red.” ===\n",
            "\n",
            "Model: base\n",
            "Response:\n",
            "Translate the following English sentence into Spanish: “The school is painted red.” “The school is painted red.” Spanish: “La escuela es pintada de rojo.” \n",
            "\n",
            "## Step 1: Identify the key elements of the sentence\n",
            "The sentence is: \"The school is painted red.\" The key elements are the subject (\"the school\"), the verb (\"is painted\"), and the object (\"red\").\n",
            "\n",
            "## Step 2: Translate the verb into Spanish\n",
            "The verb \"is painted\" in Spanish is \"es pintada.\"\n",
            "\n",
            "## Step 3: Translate the subject into Spanish\n",
            "The subject \"the school\" in Spanish is \"la escuela.\"\n",
            "\n",
            "## Step 4: Translate the object into Spanish\n",
            "The object \"red\" in Spanish is \"rojo.\"\n",
            "\n",
            "## Step 5: Combine the translated elements\n",
            "Combining the translated elements, we get \"La escuela es pintada de rojo.\"\n",
            "\n",
            "The final answer is: La escuela es pintada de rojo.\n",
            "\n",
            "Model: sft\n",
            "Response:\n",
            "Translate the following English sentence into Spanish: “The school is painted red.”. \n",
            "\n",
            "The translation is: “La escuela está pintada en rojo.”. \n",
            "\n",
            "Explanation: The word “school” is translated to “escuela”, and the verb “is painted” is translated to “está pintada” (the verb “is” is translated to “está”, and the verb “painted” is translated to “pintada”). The word “red” is translated to “rojo”. \n",
            "\n",
            "Step 1:  To translate the given English sentence, we need to identify the main elements: the school and the color red.\n",
            "Step 2:  The word “school” is translated to its Spanish equivalent, which is “escuela”.\n",
            "Step 3:  Next, we translate the verb phrase “is painted red” into its Spanish equivalent. The verb “is” is translated to “está”, and the verb “painted” is translated to “pintada”. Additionally, the word “red” is translated to its Spanish equivalent, which is “rojo”.\n",
            "Step 4:  Putting it all together, the translated sentence is: “La escuela está pintada en rojo.”. \n",
            "\n",
            "Answer: “La escuela está pintada en rojo.”.\n",
            "\n",
            "=== Question 6: If an animal barks, is it a dog or a cat? Explain your reasoning. ===\n",
            "\n",
            "Model: base\n",
            "Response:\n",
            "If an animal barks, is it a dog or a cat? Explain your reasoning. \n",
            "In this case, the answer is clearly a dog. This is because barking is a characteristic sound that is most commonly associated with dogs. Cats, on the other hand, are known for their meowing. So, based on the information given, it can be reasonably concluded that the animal is a dog.\n",
            "This type of reasoning is called a categorical syllogism, which involves making a conclusion based on two pieces of information. The first piece of information is that the animal barks, and the second piece of information is that barking is a characteristic sound of dogs. Based on these two pieces of information, the conclusion can be drawn that the animal is a dog. \n",
            "\n",
            "It's worth noting that this type of reasoning can be problematic in real-world situations, as it assumes that the only possible explanation is the one that is most likely to be true. However, in this case, the reasoning is based on a clear and widely accepted association between barking and dogs, making it a reasonable conclusion. \n",
            "\n",
            "In other words, the reasoning is sound because it is based on a well-established pattern or rule in the world. This is a key aspect of inductive reasoning, which involves making generalizations based on specific observations. In this case, the observation is that barking is characteristic of dogs, and the generalization is that the animal is a dog. \n",
            "\n",
            "Overall, the reasoning in this case is clear and logical, and it is a good example of how inductive reasoning can be used to make a conclusion based on a set of observations. \n",
            "\n",
            "Note: The above response is written in a style that is typical of a reasoning textbook or educational setting, and it assumes that the student is familiar with the concept of categorical syllogism and inductive reasoning. \n",
            "\n",
            "However, in a more casual or conversational setting, the response might be written in a more straightforward and informal style, such as:\n",
            "\n",
            "\"It's pretty obvious that the animal is a dog, since barking is a trait that's commonly associated with dogs. Cats, on the other hand, are known for meowing, so it's unlikely that this is a cat. Based on these two observations, it's reasonable to conclude that the animal is a dog.\" \n",
            "\n",
            "This version is more concise and informal, but it still conveys the same basic idea and reasoning as the original response. \n",
            "\n",
            "In a more advanced or technical setting, the response might be written in a more formal and technical style, such as:\n",
            "\n",
            "\"The categorical syllogism employed in this reasoning is based on\n",
            "\n",
            "Model: sft\n",
            "Response:\n",
            "If an animal barks, is it a dog or a cat? Explain your reasoning. \n",
            "The answer is: It's a dog! \n",
            "\n",
            "Why? Because barking is a characteristic typically associated with dogs, not cats. So, when we hear a barking sound, we immediately think of a dog. It's a common association that has been learned through experience and media.\n",
            "\n",
            "In this case, the question is trying to trick us into thinking of a cat, but our common knowledge and experience tell us that barking is a characteristic typically associated with dogs. Therefore, the answer is a dog!\n",
            "\n",
            "This example illustrates the power of associations and common knowledge in decision-making. Our brains are wired to recognize patterns and make connections between pieces of information. In this case, the association between barking and dogs is so strong that it helps us make an immediate decision. It's a great example of how our brains can use common knowledge to guide our decisions! \n",
            "\n",
            "Now, let's try another example. If an animal purrs, is it a cat or a dog? Explain your reasoning. \n",
            "\n",
            "The answer is: It's a cat! \n",
            "\n",
            "Why? Because purring is a characteristic typically associated with cats, not dogs. So, when we hear a purring sound, we immediately think of a cat. It's a common association that has been learned through experience and media.\n",
            "\n",
            "In this case, the question is trying to trick us into thinking of a dog, but our common knowledge and experience tell us that purring is a characteristic typically associated with cats. Therefore, the answer is a cat!\n",
            "\n",
            "This example illustrates the power of associations and common knowledge in decision-making. Our brains are wired to recognize patterns and make connections between pieces of information. In this case, the association between purring and cats is so strong that it helps us make an immediate decision. It's a great example of how our brains can use common knowledge to guide our decisions! \n",
            "\n",
            "Now, let's try another example. If an animal hisses, is it a cat or a dog? Explain your reasoning. \n",
            "\n",
            "The answer is: It's a cat! \n",
            "\n",
            "Why? Because hissing is a characteristic typically associated with cats, not dogs. So, when we hear a hissing sound, we immediately think of a cat. It's a common association that has been learned through experience and media.\n",
            "\n",
            "In this case, the question is trying to trick us into thinking of a dog, but our common knowledge and experience tell us that hissing is a characteristic typically associated with cats. Therefore, the answer is a cat!\n",
            "\n",
            "This example illustrates the power of associations and common knowledge in decision\n",
            "\n",
            "=== Question 7: Is the National Technical University of Athens well-established? ===\n",
            "\n",
            "Model: base\n",
            "Response:\n",
            "Is the National Technical University of Athens well-established? \n",
            "Yes, the National Technical University of Athens (NTUA) is a well-established institution of higher education in Greece. It was founded in 1836 and is one of the oldest technical universities in Europe. The university has a long history of academic excellence and has produced many notable alumni in various fields, including engineering, economics, and social sciences.\n",
            "\n",
            "NTUA is a member of the European University Association (EUA) and the Association of European Universities (AEU), and is recognized by the European Commission and the European Association of Universities (EAU). The university is also a member of the World Association of Universities and Higher Education (WAUHE).\n",
            "\n",
            "The university has a strong international reputation and has established partnerships with many universities around the world, including universities in the United States, the United Kingdom, and other European countries. NTUA is also a member of the League of European Research Universities (LERU).\n",
            "\n",
            "The university has a range of academic programs and research activities, and is recognized for its expertise in various fields, including:\n",
            "\n",
            "* Engineering (mechanical, electrical, civil, and aerospace)\n",
            "* Economics and business\n",
            "* Social sciences (sociology, psychology, and anthropology)\n",
            "* Natural sciences (physics, chemistry, and biology)\n",
            "* Mathematics and computer science\n",
            "\n",
            "NTUA has a large and diverse student body, with students from all over Greece and around the world. The university offers a range of undergraduate, graduate, and doctoral programs, as well as a range of research opportunities.\n",
            "\n",
            "Overall, the National Technical University of Athens is a well-established institution of higher education with a long history of academic excellence and a strong international reputation.\n",
            "\n",
            "Model: sft\n",
            "Response:\n",
            "Is the National Technical University of Athens well-established? The National Technical University of Athens (NTUA) is a Greek public university that was founded in 1836 as the \"School of Military Engineering\". It is one of the oldest and most prestigious universities in Greece, with a strong focus on science, technology, engineering, and mathematics (STEM) fields. The university has a long history of innovation and research, and it is a member of the Association of Universities in the European Union. It has a strong international reputation, with partnerships with top universities around the world. NTUA is known for its high-quality education and research programs, and it is a popular choice among students from all over Greece and the world. Overall, the National Technical University of Athens is a well-established and reputable institution in the field of higher education. \n",
            "\n",
            "This answer is based on the following information:\n",
            "- The university was founded in 1836 as the \"School of Military Engineering\".\n",
            "- It is one of the oldest and most prestigious universities in Greece.\n",
            "- It has a strong focus on STEM fields.\n",
            "- It is a member of the Association of Universities in the European Union.\n",
            "- It has a strong international reputation, with partnerships with top universities around the world.\n",
            "- It is known for its high-quality education and research programs.\n",
            "- It is a popular choice among students from all over Greece and the world.\n",
            "\n",
            "The answer can be supported by providing more information about the university's history, its academic programs, its research activities, and its international partnerships. Additionally, providing examples of the university's achievements and recognition in the field of higher education can also support the answer. \n",
            "\n",
            "It is worth noting that the National Technical University of Athens is a well-established institution in Greece, and it has a strong reputation in the field of higher education. However, the answer can be further supported by providing more information about the university's specific strengths and achievements. \n",
            "\n",
            "In conclusion, the National Technical University of Athens is a well-established institution in the field of higher education, with a strong focus on STEM fields and a strong international reputation. It is a popular choice among students from all over Greece and the world, and it is known for its high-quality education and research programs. Overall, the National Technical University of Athens is a reputable institution in the field of higher education. \n",
            "\n",
            "This answer is based on the following information:\n",
            "- The university was founded in 1836 as the \"School of Military Engineering\".\n",
            "- It is one of the oldest and most prestigious universities in Greece.\n",
            "- It has a strong focus on STEM fields.\n",
            "- It is a member of the Association\n"
          ]
        }
      ]
    }
  ]
}
